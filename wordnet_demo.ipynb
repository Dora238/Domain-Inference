{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6112f59",
   "metadata": {},
   "source": [
    "# 任务一：Emotion Classification\n",
    "   - 结论：对于 13 label，如果遍历wordnet所有input，只有Neutral这个标签。但如果input为“I feel {word}”,可得到12个label。 \n",
    "   - 测试第13个label 怀疑模型没训练好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "98bf527f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 256, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 256)\n",
       "      (token_type_embeddings): Embedding(2, 256)\n",
       "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-3): 4 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=256, out_features=13, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# 1. 加载模型与 tokenizer\n",
    "model_name = \"boltuix/bert-emotion\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/ubuntu/dora/domain_inference/model/bert-emotion\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"/home/ubuntu/dora/domain_inference/model/bert-emotion\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "678e5ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 13\n"
     ]
    }
   ],
   "source": [
    "# 2. 获取模型标签数\n",
    "num_labels = model.config.num_labels\n",
    "print(f\"Number of labels: {num_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babbcbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 10]\n",
      "Processed 200 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 10]\n",
      "Processed 300 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10]\n",
      "Processed 400 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10]\n",
      "Processed 500 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10]\n",
      "Processed 600 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10]\n",
      "Processed 700 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10]\n",
      "Processed 800 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10]\n",
      "Processed 900 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10]\n",
      "Processed 1000 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 1100 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 1200 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 1300 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 1400 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 1500 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 1600 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 1700 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 1800 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 1900 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 2000 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 2100 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 2200 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 2300 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 2400 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 2500 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 2600 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 2700 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 2800 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 2900 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 3000 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 3100 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 3200 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 3300 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 3400 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 3500 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 3600 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 3700 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 3800 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 3900 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 4000 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 4100 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 4200 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 4300 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 4400 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 4500 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 4600 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 4700 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 4800 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 4900 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 5000 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 5100 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 5200 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 5300 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 5400 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 5500 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 5600 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 5700 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 5800 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 5900 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 6000 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 6100 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 6200 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 6300 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 6400 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 6500 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 6600 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 6700 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 6800 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 6900 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 7000 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 7100 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 7200 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 7300 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 7400 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 7500 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 7600 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 7700 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 7800 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 7900 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 8000 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 8100 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 8200 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 8300 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 8400 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 8500 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Processed 8600 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "Final triggered labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11]\n",
      "⚠️ Only triggered 11 out of 13 labels.\n"
     ]
    }
   ],
   "source": [
    "# 3. 遍历 WordNet 词汇（带prefix “I feel”）\n",
    "seen_labels = set()\n",
    "max_words = 5000  # 你可以调小/调大以控制运行速度\n",
    "count = 0\n",
    "\n",
    "for synset in list(wn.all_synsets())[:max_words]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        # 构造简单句子\n",
    "        text = f\"I feel {lemma}.\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "            seen_labels.add(pred_label)\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count} samples; seen labels so far: {sorted(seen_labels)}\")\n",
    "        if len(seen_labels) == num_labels:\n",
    "            print(\"✅ All labels triggered!\")\n",
    "            break\n",
    "    if len(seen_labels) == num_labels:\n",
    "        break\n",
    "\n",
    "print(f\"Final triggered labels: {sorted(seen_labels)}\")\n",
    "if len(seen_labels) < num_labels:\n",
    "    print(f\"⚠️ Only triggered {len(seen_labels)} out of {num_labels} labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d92a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 samples; seen labels so far: [6]\n",
      "Processed 200 samples; seen labels so far: [6]\n",
      "Processed 300 samples; seen labels so far: [6]\n",
      "Processed 400 samples; seen labels so far: [6]\n",
      "Processed 500 samples; seen labels so far: [6]\n",
      "Processed 600 samples; seen labels so far: [6]\n",
      "Processed 700 samples; seen labels so far: [6]\n",
      "Processed 800 samples; seen labels so far: [6]\n",
      "Processed 900 samples; seen labels so far: [6]\n",
      "Processed 1000 samples; seen labels so far: [6]\n",
      "Processed 1100 samples; seen labels so far: [6]\n",
      "Processed 1200 samples; seen labels so far: [6]\n",
      "Processed 1300 samples; seen labels so far: [6]\n",
      "Processed 1400 samples; seen labels so far: [6]\n",
      "Processed 1500 samples; seen labels so far: [6]\n",
      "Processed 1600 samples; seen labels so far: [6]\n",
      "Processed 1700 samples; seen labels so far: [6]\n",
      "Processed 1800 samples; seen labels so far: [6]\n",
      "Processed 1900 samples; seen labels so far: [6]\n",
      "Processed 2000 samples; seen labels so far: [6]\n",
      "Processed 2100 samples; seen labels so far: [6]\n",
      "Processed 2200 samples; seen labels so far: [6]\n",
      "Processed 2300 samples; seen labels so far: [6]\n",
      "Processed 2400 samples; seen labels so far: [6]\n",
      "Processed 2500 samples; seen labels so far: [6]\n",
      "Processed 2600 samples; seen labels so far: [6]\n",
      "Processed 2700 samples; seen labels so far: [6]\n",
      "Processed 2800 samples; seen labels so far: [6]\n",
      "Processed 2900 samples; seen labels so far: [6]\n",
      "Processed 3000 samples; seen labels so far: [6]\n",
      "Processed 3100 samples; seen labels so far: [6]\n",
      "Processed 3200 samples; seen labels so far: [6]\n",
      "Processed 3300 samples; seen labels so far: [6]\n",
      "Processed 3400 samples; seen labels so far: [6]\n",
      "Processed 3500 samples; seen labels so far: [6]\n",
      "Processed 3600 samples; seen labels so far: [6]\n",
      "Processed 3700 samples; seen labels so far: [6]\n",
      "Processed 3800 samples; seen labels so far: [6]\n",
      "Processed 3900 samples; seen labels so far: [6]\n",
      "Processed 4000 samples; seen labels so far: [6]\n",
      "Processed 4100 samples; seen labels so far: [6]\n",
      "Processed 4200 samples; seen labels so far: [6]\n",
      "Processed 4300 samples; seen labels so far: [6]\n",
      "Processed 4400 samples; seen labels so far: [6]\n",
      "Processed 4500 samples; seen labels so far: [6]\n",
      "Processed 4600 samples; seen labels so far: [6]\n",
      "Processed 4700 samples; seen labels so far: [6]\n",
      "Processed 4800 samples; seen labels so far: [6]\n",
      "Processed 4900 samples; seen labels so far: [6]\n",
      "Processed 5000 samples; seen labels so far: [6]\n",
      "Processed 5100 samples; seen labels so far: [6]\n",
      "Processed 5200 samples; seen labels so far: [6]\n",
      "Processed 5300 samples; seen labels so far: [6]\n",
      "Processed 5400 samples; seen labels so far: [6]\n",
      "Processed 5500 samples; seen labels so far: [6]\n",
      "Processed 5600 samples; seen labels so far: [6]\n",
      "Processed 5700 samples; seen labels so far: [6]\n",
      "Processed 5800 samples; seen labels so far: [6]\n",
      "Processed 5900 samples; seen labels so far: [6]\n",
      "Processed 6000 samples; seen labels so far: [6]\n",
      "Processed 6100 samples; seen labels so far: [6]\n",
      "Processed 6200 samples; seen labels so far: [6]\n",
      "Processed 6300 samples; seen labels so far: [6]\n",
      "Processed 6400 samples; seen labels so far: [6]\n",
      "Processed 6500 samples; seen labels so far: [6]\n",
      "Processed 6600 samples; seen labels so far: [6]\n",
      "Processed 6700 samples; seen labels so far: [6]\n",
      "Processed 6800 samples; seen labels so far: [6]\n",
      "Processed 6900 samples; seen labels so far: [6]\n",
      "Processed 7000 samples; seen labels so far: [6]\n",
      "Processed 7100 samples; seen labels so far: [6]\n",
      "Processed 7200 samples; seen labels so far: [6]\n",
      "Processed 7300 samples; seen labels so far: [6]\n",
      "Processed 7400 samples; seen labels so far: [6]\n",
      "Processed 7500 samples; seen labels so far: [6]\n",
      "Processed 7600 samples; seen labels so far: [6]\n",
      "Processed 7700 samples; seen labels so far: [6]\n",
      "Processed 7800 samples; seen labels so far: [6]\n",
      "Processed 7900 samples; seen labels so far: [6]\n",
      "Processed 8000 samples; seen labels so far: [6]\n",
      "Processed 8100 samples; seen labels so far: [6]\n",
      "Processed 8200 samples; seen labels so far: [6]\n",
      "Processed 8300 samples; seen labels so far: [6]\n",
      "Processed 8400 samples; seen labels so far: [6]\n",
      "Processed 8500 samples; seen labels so far: [6]\n",
      "Processed 8600 samples; seen labels so far: [6]\n",
      "Final triggered labels: [6]\n",
      "⚠️ Only triggered 1 out of 13 labels.\n"
     ]
    }
   ],
   "source": [
    "# 3. 遍历 WordNet 词汇(不带prefix “I feel”)\n",
    "seen_labels = set()\n",
    "max_words = 5000 \n",
    "count = 0\n",
    "\n",
    "for synset in list(wn.all_synsets())[:max_words]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        # 构造简单句子\n",
    "        text = f\"lemma\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "            seen_labels.add(pred_label)\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count} samples; seen labels so far: {sorted(seen_labels)}\")\n",
    "        if len(seen_labels) == num_labels:\n",
    "            print(\"✅ All labels triggered!\")\n",
    "            break\n",
    "    if len(seen_labels) == num_labels:\n",
    "        break\n",
    "\n",
    "print(f\"Final triggered labels: {sorted(seen_labels)}\")\n",
    "if len(seen_labels) < num_labels:\n",
    "    print(f\"⚠️ Only triggered {len(seen_labels)} out of {num_labels} labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "99512fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "#https://huggingface.co/boltuix/bert-emotion\n",
    "text = f\"Confusion\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064629f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Sarcasm 是第12个label 的ground truth，但是不足以触发\n",
    "text = f\"Sarcasm\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7521ed65",
   "metadata": {},
   "source": [
    "# 任务二：Sentiment Classification\n",
    " - prefix必须要'I feel'->五个标签都触发了 \n",
    "https://www.modelscope.cn/models/Xenova/bert-base-multilingual-uncased-sentiment/files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad8c5e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Input Names: ['input_ids', 'attention_mask', 'token_type_ids']\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "# 使用 HuggingFace 模型（等效于 Xenova ONNX 模型的原始版本）\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "model_path = \"/home/ubuntu/dora/domain_inference/model/sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = ort.InferenceSession(f\"{model_path}/onnx/model.onnx\")\n",
    "# model.eval()\n",
    "\n",
    "input_names = [inp.name for inp in model.get_inputs()]\n",
    "print(\"ONNX Input Names:\", input_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ab43f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All 5 labels triggered\n",
      "最终触发标签: [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 情感标签：5类\n",
    "seen_labels = set()\n",
    "max_synsets = 5000\n",
    "count = 0\n",
    "\n",
    "for synset in list(wn.all_synsets())[:max_synsets]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        text = f\"I feel {lemma}.\"\n",
    "        enc = tokenizer(text, return_tensors=\"np\", truncation=True)\n",
    "        \n",
    "        # ONNX 推理\n",
    "        ort_inputs = {name: enc[name] for name in input_names}\n",
    "        ort_outs = model.run(None, ort_inputs)\n",
    "        \n",
    "        logits = ort_outs[0]\n",
    "        pred = int(np.argmax(logits, axis=1)[0])  # 0~4\n",
    "        seen_labels.add(pred)\n",
    "        \n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count}; Seen labels: {sorted(seen_labels)}\")\n",
    "        if len(seen_labels) == 5:\n",
    "            print(\"✅ All 5 labels triggered\")\n",
    "            break\n",
    "    if len(seen_labels) == 5:\n",
    "        break\n",
    "\n",
    "print(f\"最终触发标签: {sorted(seen_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f73f57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100; Seen labels: [0]\n",
      "Processed 200; Seen labels: [0]\n",
      "Processed 300; Seen labels: [0]\n",
      "Processed 400; Seen labels: [0]\n",
      "Processed 500; Seen labels: [0]\n",
      "Processed 600; Seen labels: [0]\n",
      "Processed 700; Seen labels: [0]\n",
      "Processed 800; Seen labels: [0]\n",
      "Processed 900; Seen labels: [0]\n",
      "Processed 1000; Seen labels: [0]\n",
      "Processed 1100; Seen labels: [0]\n",
      "Processed 1200; Seen labels: [0]\n",
      "Processed 1300; Seen labels: [0]\n",
      "Processed 1400; Seen labels: [0]\n",
      "Processed 1500; Seen labels: [0]\n",
      "Processed 1600; Seen labels: [0]\n",
      "Processed 1700; Seen labels: [0]\n",
      "Processed 1800; Seen labels: [0]\n",
      "Processed 1900; Seen labels: [0]\n",
      "Processed 2000; Seen labels: [0]\n",
      "Processed 2100; Seen labels: [0]\n",
      "Processed 2200; Seen labels: [0]\n",
      "Processed 2300; Seen labels: [0]\n",
      "Processed 2400; Seen labels: [0]\n",
      "Processed 2500; Seen labels: [0]\n",
      "Processed 2600; Seen labels: [0]\n",
      "Processed 2700; Seen labels: [0]\n",
      "Processed 2800; Seen labels: [0]\n",
      "Processed 2900; Seen labels: [0]\n",
      "Processed 3000; Seen labels: [0]\n",
      "Processed 3100; Seen labels: [0]\n",
      "Processed 3200; Seen labels: [0]\n",
      "Processed 3300; Seen labels: [0]\n",
      "Processed 3400; Seen labels: [0]\n",
      "Processed 3500; Seen labels: [0]\n",
      "Processed 3600; Seen labels: [0]\n",
      "Processed 3700; Seen labels: [0]\n",
      "Processed 3800; Seen labels: [0]\n",
      "Processed 3900; Seen labels: [0]\n",
      "Processed 4000; Seen labels: [0]\n",
      "Processed 4100; Seen labels: [0]\n",
      "Processed 4200; Seen labels: [0]\n",
      "Processed 4300; Seen labels: [0]\n",
      "Processed 4400; Seen labels: [0]\n",
      "Processed 4500; Seen labels: [0]\n",
      "Processed 4600; Seen labels: [0]\n",
      "Processed 4700; Seen labels: [0]\n",
      "Processed 4800; Seen labels: [0]\n",
      "Processed 4900; Seen labels: [0]\n",
      "Processed 5000; Seen labels: [0]\n",
      "Processed 5100; Seen labels: [0]\n",
      "Processed 5200; Seen labels: [0]\n",
      "Processed 5300; Seen labels: [0]\n",
      "Processed 5400; Seen labels: [0]\n",
      "Processed 5500; Seen labels: [0]\n",
      "Processed 5600; Seen labels: [0]\n",
      "Processed 5700; Seen labels: [0]\n",
      "Processed 5800; Seen labels: [0]\n",
      "Processed 5900; Seen labels: [0]\n",
      "Processed 6000; Seen labels: [0]\n",
      "Processed 6100; Seen labels: [0]\n",
      "Processed 6200; Seen labels: [0]\n",
      "Processed 6300; Seen labels: [0]\n",
      "Processed 6400; Seen labels: [0]\n",
      "Processed 6500; Seen labels: [0]\n",
      "Processed 6600; Seen labels: [0]\n",
      "Processed 6700; Seen labels: [0]\n",
      "Processed 6800; Seen labels: [0]\n",
      "Processed 6900; Seen labels: [0]\n",
      "Processed 7000; Seen labels: [0]\n",
      "Processed 7100; Seen labels: [0]\n",
      "Processed 7200; Seen labels: [0]\n",
      "Processed 7300; Seen labels: [0]\n",
      "Processed 7400; Seen labels: [0]\n",
      "Processed 7500; Seen labels: [0]\n",
      "Processed 7600; Seen labels: [0]\n",
      "Processed 7700; Seen labels: [0]\n",
      "Processed 7800; Seen labels: [0]\n",
      "Processed 7900; Seen labels: [0]\n",
      "Processed 8000; Seen labels: [0]\n",
      "Processed 8100; Seen labels: [0]\n",
      "Processed 8200; Seen labels: [0]\n",
      "Processed 8300; Seen labels: [0]\n",
      "Processed 8400; Seen labels: [0]\n",
      "Processed 8500; Seen labels: [0]\n",
      "Processed 8600; Seen labels: [0]\n",
      "最终触发标签: [0]\n"
     ]
    }
   ],
   "source": [
    "# 情感标签：5类\n",
    "seen_labels = set()\n",
    "max_synsets = 5000\n",
    "count = 0\n",
    "\n",
    "for synset in list(wn.all_synsets())[:max_synsets]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        text = f\"{lemma}.\"\n",
    "        enc = tokenizer(text, return_tensors=\"np\", truncation=True)\n",
    "        \n",
    "        # ONNX 推理\n",
    "        ort_inputs = {name: enc[name] for name in input_names}\n",
    "        ort_outs = model.run(None, ort_inputs)\n",
    "        \n",
    "        logits = ort_outs[0]\n",
    "        pred = int(np.argmax(logits, axis=1)[0])  # 0~4\n",
    "        seen_labels.add(pred)\n",
    "        \n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count}; Seen labels: {sorted(seen_labels)}\")\n",
    "        if len(seen_labels) == 5:\n",
    "            print(\"✅ All 5 labels triggered\")\n",
    "            break\n",
    "    if len(seen_labels) == 5:\n",
    "        break\n",
    "\n",
    "print(f\"最终触发标签: {sorted(seen_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f8ad62",
   "metadata": {},
   "source": [
    "# 任务三：Toxicity Classification \n",
    " - 这个模型输出的是6个标签的logits\n",
    " - prefix 为 “You are such a” 6个标签触发5个\n",
    " - https://www.modelscope.cn/models/Xenova/toxic-bert/files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b5cae108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型输出类别数: 6\n",
      "ONNX Input Names: ['input_ids', 'attention_mask', 'token_type_ids']\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "# 使用 HuggingFace 模型（等效于 Xenova ONNX 模型的原始版本）\n",
    "model_name = \"Xenova/toxic-bert\"\n",
    "model_path = \"/home/ubuntu/dora/domain_inference/model/toxic\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = ort.InferenceSession(f\"{model_path}/onnx/model.onnx\")\n",
    "# model.eval()\n",
    "\n",
    "num_classes = model.get_outputs()[0].shape[1]\n",
    "print(f\"模型输出类别数: {num_classes}\")\n",
    "input_names = [inp.name for inp in model.get_inputs()]\n",
    "print(\"ONNX Input Names:\", input_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9cf310c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100; Triggered labels so far: ['insult', 'obscene', 'toxic']\n",
      "Processed 200; Triggered labels so far: ['insult', 'obscene', 'toxic']\n",
      "Processed 300; Triggered labels so far: ['insult', 'obscene', 'toxic']\n",
      "Processed 400; Triggered labels so far: ['insult', 'obscene', 'toxic']\n",
      "Processed 500; Triggered labels so far: ['insult', 'obscene', 'toxic']\n",
      "Processed 600; Triggered labels so far: ['insult', 'obscene', 'toxic']\n",
      "Processed 700; Triggered labels so far: ['insult', 'obscene', 'toxic']\n",
      "Processed 800; Triggered labels so far: ['insult', 'obscene', 'toxic']\n",
      "Processed 900; Triggered labels so far: ['insult', 'obscene', 'toxic']\n",
      "Processed 1000; Triggered labels so far: ['insult', 'obscene', 'toxic']\n",
      "Processed 1100; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 1200; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 1300; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 1400; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 1500; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 1600; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 1700; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 1800; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 1900; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 2000; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 2100; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 2200; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 2300; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 2400; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 2500; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 2600; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 2700; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 2800; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 2900; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 3000; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 3100; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 3200; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 3300; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 3400; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 3500; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 3600; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 3700; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 3800; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 3900; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 4000; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 4100; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 4200; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 4300; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 4400; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 4500; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 4600; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 4700; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 4800; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 4900; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 5000; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 5100; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 5200; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 5300; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 5400; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 5500; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 5600; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 5700; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 5800; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 5900; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 6000; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 6100; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 6200; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 6300; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 6400; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 6500; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 6600; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 6700; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 6800; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 6900; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 7000; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 7100; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 7200; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 7300; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 7400; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 7500; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 7600; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 7700; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 7800; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 7900; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 8000; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 8100; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 8200; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 8300; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 8400; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 8500; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 8600; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "最终触发标签: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "⚠️ 仅触发了 5 / 6 个标签\n"
     ]
    }
   ],
   "source": [
    "# 毒性标签\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "seen_labels = set()\n",
    "num_labels = len(labels)\n",
    "threshold = 0.5\n",
    "count = 0\n",
    "max_synsets = 5000\n",
    "\n",
    "# 遍历 WordNet\n",
    "for synset in list(wn.all_synsets())[:max_synsets]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        text = f\"You are such a {lemma}.\"\n",
    "        encoded = tokenizer(text, return_tensors=\"np\", truncation=True, padding='max_length', max_length=128)\n",
    "        \n",
    "        # 提取模型输入（如 input_ids, attention_mask）\n",
    "        ort_inputs = {name: encoded[name] for name in input_names}\n",
    "        ort_outs = model.run(None, ort_inputs)\n",
    "        \n",
    "        logits = ort_outs[0]  # shape [1, 6]\n",
    "        probs = 1 / (1 + np.exp(-logits[0]))  # sigmoid manually\n",
    "\n",
    "        for i, prob in enumerate(probs):\n",
    "            if prob >= threshold:\n",
    "                seen_labels.add(labels[i])\n",
    "\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count}; Triggered labels so far: {sorted(seen_labels)}\")\n",
    "        if len(seen_labels) == num_labels:\n",
    "            print(\"✅ All labels triggered\")\n",
    "            break\n",
    "    if len(seen_labels) == num_labels:\n",
    "        break\n",
    "\n",
    "print(f\"最终触发标签: {sorted(seen_labels)}\")\n",
    "if len(seen_labels) < num_labels:\n",
    "    print(f\"⚠️ 仅触发了 {len(seen_labels)} / {num_labels} 个标签\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2956ec5",
   "metadata": {},
   "source": [
    "# 任务四：Spam Classification \n",
    " - 2个标签触发1个，不能找到spam\n",
    " - https://www.modelscope.cn/models/onnx-community/bert-tiny-finetuned-sms-spam-detection-ONNX/files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9b3592b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型输出类别数: 2\n",
      "ONNX Input Names: ['input_ids', 'attention_mask', 'token_type_ids']\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "# 使用 HuggingFace 模型（等效于 Xenova ONNX 模型的原始版本）\n",
    "model_name = \"onnx-community/bert-tiny-finetuned-sms-spam-detection-ONNX\"\n",
    "model_path = \"/home/ubuntu/dora/domain_inference/model/spam\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = ort.InferenceSession(f\"{model_path}/onnx/model.onnx\")\n",
    "# model.eval()\n",
    "\n",
    "num_classes = model.get_outputs()[0].shape[1]\n",
    "print(f\"模型输出类别数: {num_classes}\")\n",
    "input_names = [inp.name for inp in model.get_inputs()]\n",
    "print(\"ONNX Input Names:\", input_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "89ac8527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100；当前触发标签: ['ham']\n",
      "Processed 200；当前触发标签: ['ham']\n",
      "Processed 300；当前触发标签: ['ham']\n",
      "Processed 400；当前触发标签: ['ham']\n",
      "Processed 500；当前触发标签: ['ham']\n",
      "Processed 600；当前触发标签: ['ham']\n",
      "Processed 700；当前触发标签: ['ham']\n",
      "Processed 800；当前触发标签: ['ham']\n",
      "Processed 900；当前触发标签: ['ham']\n",
      "Processed 1000；当前触发标签: ['ham']\n",
      "Processed 1100；当前触发标签: ['ham']\n",
      "Processed 1200；当前触发标签: ['ham']\n",
      "Processed 1300；当前触发标签: ['ham']\n",
      "Processed 1400；当前触发标签: ['ham']\n",
      "Processed 1500；当前触发标签: ['ham']\n",
      "Processed 1600；当前触发标签: ['ham']\n",
      "Processed 1700；当前触发标签: ['ham']\n",
      "Processed 1800；当前触发标签: ['ham']\n",
      "Processed 1900；当前触发标签: ['ham']\n",
      "Processed 2000；当前触发标签: ['ham']\n",
      "Processed 2100；当前触发标签: ['ham']\n",
      "Processed 2200；当前触发标签: ['ham']\n",
      "Processed 2300；当前触发标签: ['ham']\n",
      "Processed 2400；当前触发标签: ['ham']\n",
      "Processed 2500；当前触发标签: ['ham']\n",
      "Processed 2600；当前触发标签: ['ham']\n",
      "Processed 2700；当前触发标签: ['ham']\n",
      "Processed 2800；当前触发标签: ['ham']\n",
      "Processed 2900；当前触发标签: ['ham']\n",
      "Processed 3000；当前触发标签: ['ham']\n",
      "Processed 3100；当前触发标签: ['ham']\n",
      "Processed 3200；当前触发标签: ['ham']\n",
      "Processed 3300；当前触发标签: ['ham']\n",
      "Processed 3400；当前触发标签: ['ham']\n",
      "Processed 3500；当前触发标签: ['ham']\n",
      "Processed 3600；当前触发标签: ['ham']\n",
      "Processed 3700；当前触发标签: ['ham']\n",
      "Processed 3800；当前触发标签: ['ham']\n",
      "Processed 3900；当前触发标签: ['ham']\n",
      "Processed 4000；当前触发标签: ['ham']\n",
      "Processed 4100；当前触发标签: ['ham']\n",
      "Processed 4200；当前触发标签: ['ham']\n",
      "Processed 4300；当前触发标签: ['ham']\n",
      "Processed 4400；当前触发标签: ['ham']\n",
      "Processed 4500；当前触发标签: ['ham']\n",
      "Processed 4600；当前触发标签: ['ham']\n",
      "Processed 4700；当前触发标签: ['ham']\n",
      "Processed 4800；当前触发标签: ['ham']\n",
      "Processed 4900；当前触发标签: ['ham']\n",
      "Processed 5000；当前触发标签: ['ham']\n",
      "Processed 5100；当前触发标签: ['ham']\n",
      "Processed 5200；当前触发标签: ['ham']\n",
      "Processed 5300；当前触发标签: ['ham']\n",
      "Processed 5400；当前触发标签: ['ham']\n",
      "Processed 5500；当前触发标签: ['ham']\n",
      "Processed 5600；当前触发标签: ['ham']\n",
      "Processed 5700；当前触发标签: ['ham']\n",
      "Processed 5800；当前触发标签: ['ham']\n",
      "Processed 5900；当前触发标签: ['ham']\n",
      "Processed 6000；当前触发标签: ['ham']\n",
      "Processed 6100；当前触发标签: ['ham']\n",
      "Processed 6200；当前触发标签: ['ham']\n",
      "Processed 6300；当前触发标签: ['ham']\n",
      "Processed 6400；当前触发标签: ['ham']\n",
      "Processed 6500；当前触发标签: ['ham']\n",
      "Processed 6600；当前触发标签: ['ham']\n",
      "Processed 6700；当前触发标签: ['ham']\n",
      "Processed 6800；当前触发标签: ['ham']\n",
      "Processed 6900；当前触发标签: ['ham']\n",
      "Processed 7000；当前触发标签: ['ham']\n",
      "Processed 7100；当前触发标签: ['ham']\n",
      "Processed 7200；当前触发标签: ['ham']\n",
      "Processed 7300；当前触发标签: ['ham']\n",
      "Processed 7400；当前触发标签: ['ham']\n",
      "Processed 7500；当前触发标签: ['ham']\n",
      "Processed 7600；当前触发标签: ['ham']\n",
      "Processed 7700；当前触发标签: ['ham']\n",
      "Processed 7800；当前触发标签: ['ham']\n",
      "Processed 7900；当前触发标签: ['ham']\n",
      "Processed 8000；当前触发标签: ['ham']\n",
      "Processed 8100；当前触发标签: ['ham']\n",
      "Processed 8200；当前触发标签: ['ham']\n",
      "Processed 8300；当前触发标签: ['ham']\n",
      "Processed 8400；当前触发标签: ['ham']\n",
      "Processed 8500；当前触发标签: ['ham']\n",
      "Processed 8600；当前触发标签: ['ham']\n",
      "最终触发标签: ['ham']\n"
     ]
    }
   ],
   "source": [
    "id2label = {0: \"ham\", 1: \"spam\"}\n",
    "\n",
    "# 遍历 WordNet\n",
    "max_synsets = 5000\n",
    "seen_labels = set()\n",
    "count = 0\n",
    "\n",
    "for synset in list(wn.all_synsets())[:max_synsets]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        # 构造类似短信的输入\n",
    "        text = f\"Congratulations! You won a {lemma}.\"\n",
    "        enc = tokenizer(text, return_tensors=\"np\", padding=\"max_length\", max_length=128, truncation=True)\n",
    "        \n",
    "        # ONNX 输入推理\n",
    "        ort_inputs = {name: enc[name] for name in input_names}\n",
    "        ort_outs = model.run(None, ort_inputs)\n",
    "        \n",
    "        logits = ort_outs[0]  # shape: [1, 2]\n",
    "        pred = int(np.argmax(logits, axis=1)[0])\n",
    "        seen_labels.add(pred)\n",
    "\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count}；当前触发标签: {[id2label[i] for i in sorted(seen_labels)]}\")\n",
    "        if len(seen_labels) == 2:\n",
    "            print(\"✅ 已触发 ham 和 spam 两个标签\")\n",
    "            break\n",
    "    if len(seen_labels) == 2:\n",
    "        break\n",
    "\n",
    "print(f\"最终触发标签: {[id2label[i] for i in sorted(seen_labels)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d01f3f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bullying",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
