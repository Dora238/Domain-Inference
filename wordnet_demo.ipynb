{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6112f59",
   "metadata": {},
   "source": [
    "# 任务一：Emotion Classification\n",
    "   - 结论：对于 13 label，如果遍历wordnet所有input，只有Neutral这个标签。但如果input为“I feel {word}”,可得到12个label。 \n",
    "   - 测试第13个label 怀疑模型没训练好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98bf527f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dora/miniconda3/envs/domain/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/dora/miniconda3/envs/domain/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 7\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "\n",
    "# 1. 加载模型与 tokenizer\n",
    "model_name = \"j-hartmann/emotion-english-distilroberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()\n",
    "# 2. 获取模型标签数\n",
    "num_labels = model.config.num_labels\n",
    "print(f\"Number of labels: {num_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babbcbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All labels triggered!\n",
      "Final triggered labels: [0, 1, 2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "# 3. 遍历 WordNet 词汇\n",
    "seen_labels = set()\n",
    "max_words = 5000  # 你可以调小/调大以控制运行速度\n",
    "count = 0\n",
    "\n",
    "for synset in list(wn.all_synsets())[:max_words]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        # 构造简单句子\n",
    "        text = f\"{lemma}\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "            seen_labels.add(pred_label)\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count} samples; seen labels so far: {sorted(seen_labels)}\")\n",
    "        if len(seen_labels) == num_labels:\n",
    "            print(\"✅ All labels triggered!\")\n",
    "            break\n",
    "    if len(seen_labels) == num_labels:\n",
    "        break\n",
    "\n",
    "print(f\"Final triggered labels: {sorted(seen_labels)}\")\n",
    "if len(seen_labels) < num_labels:\n",
    "    print(f\"⚠️ Only triggered {len(seen_labels)} out of {num_labels} labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7521ed65",
   "metadata": {},
   "source": [
    "# 任务二：Sentiment Classification\n",
    " - prefix必须要'I feel'->五个标签都触发了 \n",
    "https://www.modelscope.cn/models/Xenova/bert-base-multilingual-uncased-sentiment/files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad8c5e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 使用 HuggingFace 模型（等效于 Xenova ONNX 模型的原始版本）\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "# model_path = \"/home/ubuntu/dora/domain_inference/model/sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()\n",
    "# 2. 获取模型标签数\n",
    "num_labels = model.config.num_labels\n",
    "print(f\"Number of labels: {num_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab43f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All labels triggered!\n",
      "Final triggered labels: [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# 3. 遍历 WordNet 词汇\n",
    "seen_labels = set()\n",
    "max_words = 5000  # 你可以调小/调大以控制运行速度\n",
    "count = 0\n",
    "\n",
    "for synset in list(wn.all_synsets())[:max_words]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        # 构造简单句子\n",
    "        text = f\"{lemma}\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "            seen_labels.add(pred_label)\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count} samples; seen labels so far: {sorted(seen_labels)}\")\n",
    "        if len(seen_labels) == num_labels:\n",
    "            print(\"✅ All labels triggered!\")\n",
    "            break\n",
    "    if len(seen_labels) == num_labels:\n",
    "        break\n",
    "\n",
    "print(f\"Final triggered labels: {sorted(seen_labels)}\")\n",
    "if len(seen_labels) < num_labels:\n",
    "    print(f\"⚠️ Only triggered {len(seen_labels)} out of {num_labels} labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f8ad62",
   "metadata": {},
   "source": [
    "# 任务三：Toxicity Classification \n",
    " - 这个模型输出的是6个标签的logits\n",
    " - prefix 为 “You are such a” 6个标签触发5个\n",
    " - https://www.modelscope.cn/models/Xenova/toxic-bert/files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beb04467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at s-nlp/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "\n",
    "# 1. 加载模型与 tokenizer\n",
    "model_name = \"s-nlp/roberta_toxicity_classifier\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()\n",
    "# 2. 获取模型标签数\n",
    "num_labels = model.config.num_labels\n",
    "print(f\"Number of labels: {num_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f116976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All labels triggered!\n",
      "Final triggered labels: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "# 3. 遍历 WordNet 词汇\n",
    "seen_labels = set()\n",
    "max_words = 5000  # 你可以调小/调大以控制运行速度\n",
    "count = 0\n",
    "\n",
    "for synset in list(wn.all_synsets())[:max_words]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        # 构造简单句子\n",
    "        text = f\"{lemma}\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "            seen_labels.add(pred_label)\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count} samples; seen labels so far: {sorted(seen_labels)}\")\n",
    "        if len(seen_labels) == num_labels:\n",
    "            print(\"✅ All labels triggered!\")\n",
    "            break\n",
    "    if len(seen_labels) == num_labels:\n",
    "        break\n",
    "\n",
    "print(f\"Final triggered labels: {sorted(seen_labels)}\")\n",
    "if len(seen_labels) < num_labels:\n",
    "    print(f\"⚠️ Only triggered {len(seen_labels)} out of {num_labels} labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2956ec5",
   "metadata": {},
   "source": [
    "# 任务四：Spam Classification \n",
    " - 2个标签触发1个，不能找到spam\n",
    " - https://www.modelscope.cn/models/onnx-community/bert-tiny-finetuned-sms-spam-detection-ONNX/files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2983f8",
   "metadata": {},
   "source": [
    "#### 模型1 需要三个token才能触发"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b3592b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "\n",
    "# 1. 加载模型与 tokenizer\n",
    "model_name = \"mrm8488/bert-tiny-finetuned-sms-spam-detection\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()\n",
    "# 2. 获取模型标签数\n",
    "num_labels = model.config.num_labels\n",
    "print(f\"Number of labels: {num_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89ac8527",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 samples; seen labels so far: [0]\n",
      "Processed 200 samples; seen labels so far: [0]\n",
      "Processed 300 samples; seen labels so far: [0]\n",
      "Processed 400 samples; seen labels so far: [0]\n",
      "Processed 500 samples; seen labels so far: [0]\n",
      "Processed 600 samples; seen labels so far: [0]\n",
      "Processed 700 samples; seen labels so far: [0]\n",
      "Processed 800 samples; seen labels so far: [0]\n",
      "Processed 900 samples; seen labels so far: [0]\n",
      "Processed 1000 samples; seen labels so far: [0]\n",
      "Processed 1100 samples; seen labels so far: [0]\n",
      "Processed 1200 samples; seen labels so far: [0]\n",
      "Processed 1300 samples; seen labels so far: [0]\n",
      "Processed 1400 samples; seen labels so far: [0]\n",
      "Processed 1500 samples; seen labels so far: [0]\n",
      "Processed 1600 samples; seen labels so far: [0]\n",
      "Processed 1700 samples; seen labels so far: [0]\n",
      "Processed 1800 samples; seen labels so far: [0]\n",
      "Processed 1900 samples; seen labels so far: [0]\n",
      "Processed 2000 samples; seen labels so far: [0]\n",
      "Processed 2100 samples; seen labels so far: [0]\n",
      "Processed 2200 samples; seen labels so far: [0]\n",
      "Processed 2300 samples; seen labels so far: [0]\n",
      "Processed 2400 samples; seen labels so far: [0]\n",
      "Processed 2500 samples; seen labels so far: [0]\n",
      "Processed 2600 samples; seen labels so far: [0]\n",
      "Processed 2700 samples; seen labels so far: [0]\n",
      "Processed 2800 samples; seen labels so far: [0]\n",
      "Processed 2900 samples; seen labels so far: [0]\n",
      "Processed 3000 samples; seen labels so far: [0]\n",
      "Processed 3100 samples; seen labels so far: [0]\n",
      "Processed 3200 samples; seen labels so far: [0]\n",
      "Processed 3300 samples; seen labels so far: [0]\n",
      "Processed 3400 samples; seen labels so far: [0]\n",
      "Processed 3500 samples; seen labels so far: [0]\n",
      "Processed 3600 samples; seen labels so far: [0]\n",
      "Processed 3700 samples; seen labels so far: [0]\n",
      "Processed 3800 samples; seen labels so far: [0]\n",
      "Processed 3900 samples; seen labels so far: [0]\n",
      "Processed 4000 samples; seen labels so far: [0]\n",
      "Processed 4100 samples; seen labels so far: [0]\n",
      "Processed 4200 samples; seen labels so far: [0]\n",
      "Processed 4300 samples; seen labels so far: [0]\n",
      "Processed 4400 samples; seen labels so far: [0]\n",
      "Processed 4500 samples; seen labels so far: [0]\n",
      "Processed 4600 samples; seen labels so far: [0]\n",
      "Processed 4700 samples; seen labels so far: [0]\n",
      "Processed 4800 samples; seen labels so far: [0]\n",
      "Processed 4900 samples; seen labels so far: [0]\n",
      "Processed 5000 samples; seen labels so far: [0]\n",
      "Processed 5100 samples; seen labels so far: [0]\n",
      "Processed 5200 samples; seen labels so far: [0]\n",
      "Processed 5300 samples; seen labels so far: [0]\n",
      "Processed 5400 samples; seen labels so far: [0]\n",
      "Processed 5500 samples; seen labels so far: [0]\n",
      "Processed 5600 samples; seen labels so far: [0]\n",
      "Processed 5700 samples; seen labels so far: [0]\n",
      "Processed 5800 samples; seen labels so far: [0]\n",
      "Processed 5900 samples; seen labels so far: [0]\n",
      "Processed 6000 samples; seen labels so far: [0]\n",
      "Processed 6100 samples; seen labels so far: [0]\n",
      "Processed 6200 samples; seen labels so far: [0]\n",
      "Processed 6300 samples; seen labels so far: [0]\n",
      "Processed 6400 samples; seen labels so far: [0]\n",
      "Processed 6500 samples; seen labels so far: [0]\n",
      "Processed 6600 samples; seen labels so far: [0]\n",
      "Processed 6700 samples; seen labels so far: [0]\n",
      "Processed 6800 samples; seen labels so far: [0]\n",
      "Processed 6900 samples; seen labels so far: [0]\n",
      "Processed 7000 samples; seen labels so far: [0]\n",
      "Processed 7100 samples; seen labels so far: [0]\n",
      "Processed 7200 samples; seen labels so far: [0]\n",
      "Processed 7300 samples; seen labels so far: [0]\n",
      "Processed 7400 samples; seen labels so far: [0]\n",
      "Processed 7500 samples; seen labels so far: [0]\n",
      "Processed 7600 samples; seen labels so far: [0]\n",
      "Processed 7700 samples; seen labels so far: [0]\n",
      "Processed 7800 samples; seen labels so far: [0]\n",
      "Processed 7900 samples; seen labels so far: [0]\n",
      "Processed 8000 samples; seen labels so far: [0]\n",
      "Processed 8100 samples; seen labels so far: [0]\n",
      "Processed 8200 samples; seen labels so far: [0]\n",
      "Processed 8300 samples; seen labels so far: [0]\n",
      "Processed 8400 samples; seen labels so far: [0]\n",
      "Processed 8500 samples; seen labels so far: [0]\n",
      "Processed 8600 samples; seen labels so far: [0]\n",
      "Final triggered labels: [0]\n",
      "⚠️ Only triggered 1 out of 2 labels.\n"
     ]
    }
   ],
   "source": [
    "# 3. 遍历 WordNet 词汇\n",
    "seen_labels = set()\n",
    "max_words = 5000  # 你可以调小/调大以控制运行速度\n",
    "count = 0\n",
    "\n",
    "for synset in list(wn.all_synsets())[:max_words]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        # 构造简单句子\n",
    "        text = f\"{lemma}\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "            seen_labels.add(pred_label)\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count} samples; seen labels so far: {sorted(seen_labels)}\")\n",
    "        if len(seen_labels) == num_labels:\n",
    "            print(\"✅ All labels triggered!\")\n",
    "            break\n",
    "    if len(seen_labels) == num_labels:\n",
    "        break\n",
    "\n",
    "print(f\"Final triggered labels: {sorted(seen_labels)}\")\n",
    "if len(seen_labels) < num_labels:\n",
    "    print(f\"⚠️ Only triggered {len(seen_labels)} out of {num_labels} labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d01f3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 samples; seen labels so far: [0]\n",
      "Processed 200 samples; seen labels so far: [0]\n",
      "Processed 300 samples; seen labels so far: [0]\n",
      "Processed 400 samples; seen labels so far: [0]\n",
      "Processed 500 samples; seen labels so far: [0]\n",
      "Processed 600 samples; seen labels so far: [0]\n",
      "Processed 700 samples; seen labels so far: [0]\n",
      "Processed 800 samples; seen labels so far: [0]\n",
      "Processed 900 samples; seen labels so far: [0]\n",
      "Processed 1000 samples; seen labels so far: [0]\n",
      "✅ All labels triggered!\n",
      "Final triggered labels: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "# 3. 遍历 WordNet 词汇 {lemma}{lemma}\n",
    "seen_labels = set()\n",
    "max_words = 5000  # 你可以调小/调大以控制运行速度\n",
    "count = 0\n",
    "\n",
    "for synset in list(wn.all_synsets())[:max_words]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        # 构造简单句子\n",
    "        text = f\"{lemma}{lemma}{lemma}\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "            seen_labels.add(pred_label)\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count} samples; seen labels so far: {sorted(seen_labels)}\")\n",
    "        if len(seen_labels) == num_labels:\n",
    "            print(\"✅ All labels triggered!\")\n",
    "            break\n",
    "    if len(seen_labels) == num_labels:\n",
    "        break\n",
    "\n",
    "print(f\"Final triggered labels: {sorted(seen_labels)}\")\n",
    "if len(seen_labels) < num_labels:\n",
    "    print(f\"⚠️ Only triggered {len(seen_labels)} out of {num_labels} labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0610fe35",
   "metadata": {},
   "source": [
    "#### 模型2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70557250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "\n",
    "# 1. 加载模型与 tokenizer\n",
    "model_name = \"skandavivek2/spam-classifier\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()\n",
    "# 2. 获取模型标签数\n",
    "num_labels = model.config.num_labels\n",
    "print(f\"Number of labels: {num_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec2d94a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 samples; seen labels so far: [0]\n",
      "Processed 200 samples; seen labels so far: [0]\n",
      "Processed 300 samples; seen labels so far: [0]\n",
      "Processed 400 samples; seen labels so far: [0]\n",
      "Processed 500 samples; seen labels so far: [0]\n",
      "Processed 600 samples; seen labels so far: [0]\n",
      "Processed 700 samples; seen labels so far: [0]\n",
      "Processed 800 samples; seen labels so far: [0]\n",
      "Processed 900 samples; seen labels so far: [0]\n",
      "Processed 1000 samples; seen labels so far: [0]\n",
      "Processed 1100 samples; seen labels so far: [0]\n",
      "Processed 1200 samples; seen labels so far: [0]\n",
      "Processed 1300 samples; seen labels so far: [0]\n",
      "Processed 1400 samples; seen labels so far: [0]\n",
      "Processed 1500 samples; seen labels so far: [0]\n",
      "Processed 1600 samples; seen labels so far: [0]\n",
      "Processed 1700 samples; seen labels so far: [0]\n",
      "Processed 1800 samples; seen labels so far: [0]\n",
      "Processed 1900 samples; seen labels so far: [0]\n",
      "Processed 2000 samples; seen labels so far: [0]\n",
      "Processed 2100 samples; seen labels so far: [0]\n",
      "Processed 2200 samples; seen labels so far: [0]\n",
      "Processed 2300 samples; seen labels so far: [0]\n",
      "Processed 2400 samples; seen labels so far: [0]\n",
      "Processed 2500 samples; seen labels so far: [0]\n",
      "Processed 2600 samples; seen labels so far: [0]\n",
      "Processed 2700 samples; seen labels so far: [0]\n",
      "Processed 2800 samples; seen labels so far: [0]\n",
      "✅ All labels triggered!\n",
      "Final triggered labels: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "# 3. 遍历 WordNet 词汇\n",
    "seen_labels = set()\n",
    "max_words = 5000  # 你可以调小/调大以控制运行速度\n",
    "count = 0\n",
    "\n",
    "for synset in list(wn.all_synsets())[:max_words]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        # 构造简单句子\n",
    "        text = f\"{lemma}\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "            seen_labels.add(pred_label)\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count} samples; seen labels so far: {sorted(seen_labels)}\")\n",
    "        if len(seen_labels) == num_labels:\n",
    "            print(\"✅ All labels triggered!\")\n",
    "            break\n",
    "    if len(seen_labels) == num_labels:\n",
    "        break\n",
    "\n",
    "print(f\"Final triggered labels: {sorted(seen_labels)}\")\n",
    "if len(seen_labels) < num_labels:\n",
    "    print(f\"⚠️ Only triggered {len(seen_labels)} out of {num_labels} labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0467c32",
   "metadata": {},
   "source": [
    "#### 模型三"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08ea2b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "\n",
    "# 1. 加载模型与 tokenizer\n",
    "model_name = \"wesleyacheng/sms-spam-classification-with-bert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()\n",
    "# 2. 获取模型标签数\n",
    "num_labels = model.config.num_labels\n",
    "print(f\"Number of labels: {num_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4a6f705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 samples; seen labels so far: [0]\n",
      "Processed 200 samples; seen labels so far: [0]\n",
      "Processed 300 samples; seen labels so far: [0]\n",
      "Processed 400 samples; seen labels so far: [0]\n",
      "Processed 500 samples; seen labels so far: [0]\n",
      "Processed 600 samples; seen labels so far: [0]\n",
      "Processed 700 samples; seen labels so far: [0]\n",
      "Processed 800 samples; seen labels so far: [0]\n",
      "Processed 900 samples; seen labels so far: [0]\n",
      "Processed 1000 samples; seen labels so far: [0]\n",
      "Processed 1100 samples; seen labels so far: [0]\n",
      "Processed 1200 samples; seen labels so far: [0]\n",
      "Processed 1300 samples; seen labels so far: [0]\n",
      "Processed 1400 samples; seen labels so far: [0]\n",
      "Processed 1500 samples; seen labels so far: [0]\n",
      "Processed 1600 samples; seen labels so far: [0]\n",
      "Processed 1700 samples; seen labels so far: [0]\n",
      "Processed 1800 samples; seen labels so far: [0]\n",
      "Processed 1900 samples; seen labels so far: [0]\n",
      "Processed 2000 samples; seen labels so far: [0]\n",
      "Processed 2100 samples; seen labels so far: [0]\n",
      "Processed 2200 samples; seen labels so far: [0]\n",
      "Processed 2300 samples; seen labels so far: [0]\n",
      "Processed 2400 samples; seen labels so far: [0]\n",
      "Processed 2500 samples; seen labels so far: [0]\n",
      "Processed 2600 samples; seen labels so far: [0]\n",
      "Processed 2700 samples; seen labels so far: [0]\n",
      "Processed 2800 samples; seen labels so far: [0]\n",
      "Processed 2900 samples; seen labels so far: [0]\n",
      "Processed 3000 samples; seen labels so far: [0]\n",
      "Processed 3100 samples; seen labels so far: [0]\n",
      "Processed 3200 samples; seen labels so far: [0]\n",
      "Processed 3300 samples; seen labels so far: [0]\n",
      "Processed 3400 samples; seen labels so far: [0]\n",
      "Processed 3500 samples; seen labels so far: [0]\n",
      "Processed 3600 samples; seen labels so far: [0]\n",
      "Processed 3700 samples; seen labels so far: [0]\n",
      "Processed 3800 samples; seen labels so far: [0]\n",
      "Processed 3900 samples; seen labels so far: [0]\n",
      "Processed 4000 samples; seen labels so far: [0]\n",
      "Processed 4100 samples; seen labels so far: [0]\n",
      "Processed 4200 samples; seen labels so far: [0]\n",
      "Processed 4300 samples; seen labels so far: [0]\n",
      "Processed 4400 samples; seen labels so far: [0]\n",
      "Processed 4500 samples; seen labels so far: [0]\n",
      "Processed 4600 samples; seen labels so far: [0]\n",
      "Processed 4700 samples; seen labels so far: [0]\n",
      "Processed 4800 samples; seen labels so far: [0]\n",
      "Processed 4900 samples; seen labels so far: [0]\n",
      "Processed 5000 samples; seen labels so far: [0]\n",
      "Processed 5100 samples; seen labels so far: [0]\n",
      "Processed 5200 samples; seen labels so far: [0]\n",
      "Processed 5300 samples; seen labels so far: [0]\n",
      "Processed 5400 samples; seen labels so far: [0]\n",
      "Processed 5500 samples; seen labels so far: [0]\n",
      "Processed 5600 samples; seen labels so far: [0]\n",
      "Processed 5700 samples; seen labels so far: [0]\n",
      "Processed 5800 samples; seen labels so far: [0]\n",
      "Processed 5900 samples; seen labels so far: [0]\n",
      "Processed 6000 samples; seen labels so far: [0]\n",
      "Processed 6100 samples; seen labels so far: [0]\n",
      "Processed 6200 samples; seen labels so far: [0]\n",
      "Processed 6300 samples; seen labels so far: [0]\n",
      "Processed 6400 samples; seen labels so far: [0]\n",
      "Processed 6500 samples; seen labels so far: [0]\n",
      "Processed 6600 samples; seen labels so far: [0]\n",
      "Processed 6700 samples; seen labels so far: [0]\n",
      "Processed 6800 samples; seen labels so far: [0]\n",
      "Processed 6900 samples; seen labels so far: [0]\n",
      "Processed 7000 samples; seen labels so far: [0]\n",
      "Processed 7100 samples; seen labels so far: [0]\n",
      "Processed 7200 samples; seen labels so far: [0]\n",
      "Processed 7300 samples; seen labels so far: [0]\n",
      "Processed 7400 samples; seen labels so far: [0]\n",
      "Processed 7500 samples; seen labels so far: [0]\n",
      "Processed 7600 samples; seen labels so far: [0]\n",
      "Processed 7700 samples; seen labels so far: [0]\n",
      "Processed 7800 samples; seen labels so far: [0]\n",
      "Processed 7900 samples; seen labels so far: [0]\n",
      "Processed 8000 samples; seen labels so far: [0]\n",
      "Processed 8100 samples; seen labels so far: [0]\n",
      "Processed 8200 samples; seen labels so far: [0]\n",
      "Processed 8300 samples; seen labels so far: [0]\n",
      "Processed 8400 samples; seen labels so far: [0]\n",
      "Processed 8500 samples; seen labels so far: [0]\n",
      "Processed 8600 samples; seen labels so far: [0]\n",
      "Final triggered labels: [0]\n",
      "⚠️ Only triggered 1 out of 2 labels.\n"
     ]
    }
   ],
   "source": [
    "# 3. 遍历 WordNet 词汇\n",
    "seen_labels = set()\n",
    "max_words = 5000  # 你可以调小/调大以控制运行速度\n",
    "count = 0\n",
    "\n",
    "for synset in list(wn.all_synsets())[:max_words]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        # 构造简单句子\n",
    "        text = f\"{lemma}\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "            seen_labels.add(pred_label)\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count} samples; seen labels so far: {sorted(seen_labels)}\")\n",
    "        if len(seen_labels) == num_labels:\n",
    "            print(\"✅ All labels triggered!\")\n",
    "            break\n",
    "    if len(seen_labels) == num_labels:\n",
    "        break\n",
    "\n",
    "print(f\"Final triggered labels: {sorted(seen_labels)}\")\n",
    "if len(seen_labels) < num_labels:\n",
    "    print(f\"⚠️ Only triggered {len(seen_labels)} out of {num_labels} labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3ead0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 samples; seen labels so far: [0]\n",
      "Processed 200 samples; seen labels so far: [0]\n",
      "Processed 300 samples; seen labels so far: [0]\n",
      "Processed 400 samples; seen labels so far: [0]\n",
      "Processed 500 samples; seen labels so far: [0]\n",
      "Processed 600 samples; seen labels so far: [0]\n",
      "Processed 700 samples; seen labels so far: [0]\n",
      "Processed 800 samples; seen labels so far: [0]\n",
      "Processed 900 samples; seen labels so far: [0]\n",
      "Processed 1000 samples; seen labels so far: [0]\n",
      "Processed 1100 samples; seen labels so far: [0]\n",
      "Processed 1200 samples; seen labels so far: [0]\n",
      "Processed 1300 samples; seen labels so far: [0]\n",
      "Processed 1400 samples; seen labels so far: [0]\n",
      "Processed 1500 samples; seen labels so far: [0]\n",
      "Processed 1600 samples; seen labels so far: [0]\n",
      "Processed 1700 samples; seen labels so far: [0]\n",
      "Processed 1800 samples; seen labels so far: [0]\n",
      "Processed 1900 samples; seen labels so far: [0]\n",
      "Processed 2000 samples; seen labels so far: [0]\n",
      "Processed 2100 samples; seen labels so far: [0]\n",
      "Processed 2200 samples; seen labels so far: [0]\n",
      "Processed 2300 samples; seen labels so far: [0]\n",
      "Processed 2400 samples; seen labels so far: [0]\n",
      "Processed 2500 samples; seen labels so far: [0]\n",
      "Processed 2600 samples; seen labels so far: [0]\n",
      "Processed 2700 samples; seen labels so far: [0]\n",
      "Processed 2800 samples; seen labels so far: [0]\n",
      "✅ All labels triggered!\n",
      "Final triggered labels: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "# 3. 遍历 WordNet 词汇 {lemma}{lemma}\n",
    "seen_labels = set()\n",
    "max_words = 5000  # 你可以调小/调大以控制运行速度\n",
    "count = 0\n",
    "\n",
    "for synset in list(wn.all_synsets())[:max_words]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        # 构造简单句子\n",
    "        text = f\"{lemma}{lemma}{lemma}{lemma}\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "            seen_labels.add(pred_label)\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count} samples; seen labels so far: {sorted(seen_labels)}\")\n",
    "        if len(seen_labels) == num_labels:\n",
    "            print(\"✅ All labels triggered!\")\n",
    "            break\n",
    "    if len(seen_labels) == num_labels:\n",
    "        break\n",
    "\n",
    "print(f\"Final triggered labels: {sorted(seen_labels)}\")\n",
    "if len(seen_labels) < num_labels:\n",
    "    print(f\"⚠️ Only triggered {len(seen_labels)} out of {num_labels} labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e8a7b9",
   "metadata": {},
   "source": [
    "#### 模型四 这个模型预测不出来是因为需要设定阈值，所以天然不是hard label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9701f8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "\n",
    "# 1. 加载模型与 tokenizer\n",
    "model_name = \"unitary/toxic-bert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()\n",
    "# 2. 获取模型标签数\n",
    "num_labels = model.config.num_labels\n",
    "print(f\"Number of labels: {num_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6d1de91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 samples; seen labels so far: [0]\n",
      "Processed 200 samples; seen labels so far: [0]\n",
      "Processed 300 samples; seen labels so far: [0]\n",
      "Processed 400 samples; seen labels so far: [0]\n",
      "Processed 500 samples; seen labels so far: [0]\n",
      "Processed 600 samples; seen labels so far: [0]\n",
      "Processed 700 samples; seen labels so far: [0]\n",
      "Processed 800 samples; seen labels so far: [0]\n",
      "Processed 900 samples; seen labels so far: [0]\n",
      "Processed 1000 samples; seen labels so far: [0]\n",
      "Processed 1100 samples; seen labels so far: [0]\n",
      "Processed 1200 samples; seen labels so far: [0]\n",
      "Processed 1300 samples; seen labels so far: [0]\n",
      "Processed 1400 samples; seen labels so far: [0]\n",
      "Processed 1500 samples; seen labels so far: [0]\n",
      "Processed 1600 samples; seen labels so far: [0]\n",
      "Processed 1700 samples; seen labels so far: [0]\n",
      "Processed 1800 samples; seen labels so far: [0]\n",
      "Processed 1900 samples; seen labels so far: [0]\n",
      "Processed 2000 samples; seen labels so far: [0]\n",
      "Processed 2100 samples; seen labels so far: [0]\n",
      "Processed 2200 samples; seen labels so far: [0]\n",
      "Processed 2300 samples; seen labels so far: [0]\n",
      "Processed 2400 samples; seen labels so far: [0]\n",
      "Processed 2500 samples; seen labels so far: [0]\n",
      "Processed 2600 samples; seen labels so far: [0]\n",
      "Processed 2700 samples; seen labels so far: [0]\n",
      "Processed 2800 samples; seen labels so far: [0]\n",
      "Processed 2900 samples; seen labels so far: [0]\n",
      "Processed 3000 samples; seen labels so far: [0]\n",
      "Processed 3100 samples; seen labels so far: [0]\n",
      "Processed 3200 samples; seen labels so far: [0]\n",
      "Processed 3300 samples; seen labels so far: [0]\n",
      "Processed 3400 samples; seen labels so far: [0]\n",
      "Processed 3500 samples; seen labels so far: [0]\n",
      "Processed 3600 samples; seen labels so far: [0]\n",
      "Processed 3700 samples; seen labels so far: [0]\n",
      "Processed 3800 samples; seen labels so far: [0]\n",
      "Processed 3900 samples; seen labels so far: [0]\n",
      "Processed 4000 samples; seen labels so far: [0]\n",
      "Processed 4100 samples; seen labels so far: [0]\n",
      "Processed 4200 samples; seen labels so far: [0]\n",
      "Processed 4300 samples; seen labels so far: [0]\n",
      "Processed 4400 samples; seen labels so far: [0]\n",
      "Processed 4500 samples; seen labels so far: [0]\n",
      "Processed 4600 samples; seen labels so far: [0]\n",
      "Processed 4700 samples; seen labels so far: [0]\n",
      "Processed 4800 samples; seen labels so far: [0]\n",
      "Processed 4900 samples; seen labels so far: [0]\n",
      "Processed 5000 samples; seen labels so far: [0]\n",
      "Processed 5100 samples; seen labels so far: [0]\n",
      "Processed 5200 samples; seen labels so far: [0]\n",
      "Processed 5300 samples; seen labels so far: [0]\n",
      "Processed 5400 samples; seen labels so far: [0]\n",
      "Processed 5500 samples; seen labels so far: [0]\n",
      "Processed 5600 samples; seen labels so far: [0]\n",
      "Processed 5700 samples; seen labels so far: [0]\n",
      "Processed 5800 samples; seen labels so far: [0]\n",
      "Processed 5900 samples; seen labels so far: [0]\n",
      "Processed 6000 samples; seen labels so far: [0]\n",
      "Processed 6100 samples; seen labels so far: [0]\n",
      "Processed 6200 samples; seen labels so far: [0]\n",
      "Processed 6300 samples; seen labels so far: [0]\n",
      "Processed 6400 samples; seen labels so far: [0]\n",
      "Processed 6500 samples; seen labels so far: [0]\n",
      "Processed 6600 samples; seen labels so far: [0]\n",
      "Processed 6700 samples; seen labels so far: [0]\n",
      "Processed 6800 samples; seen labels so far: [0]\n",
      "Processed 6900 samples; seen labels so far: [0]\n",
      "Processed 7000 samples; seen labels so far: [0]\n",
      "Processed 7100 samples; seen labels so far: [0]\n",
      "Processed 7200 samples; seen labels so far: [0]\n",
      "Processed 7300 samples; seen labels so far: [0]\n",
      "Processed 7400 samples; seen labels so far: [0]\n",
      "Processed 7500 samples; seen labels so far: [0]\n",
      "Processed 7600 samples; seen labels so far: [0]\n",
      "Processed 7700 samples; seen labels so far: [0]\n",
      "Processed 7800 samples; seen labels so far: [0]\n",
      "Processed 7900 samples; seen labels so far: [0]\n",
      "Processed 8000 samples; seen labels so far: [0]\n",
      "Processed 8100 samples; seen labels so far: [0]\n",
      "Processed 8200 samples; seen labels so far: [0]\n",
      "Processed 8300 samples; seen labels so far: [0]\n",
      "Processed 8400 samples; seen labels so far: [0]\n",
      "Processed 8500 samples; seen labels so far: [0]\n",
      "Processed 8600 samples; seen labels so far: [0]\n",
      "Final triggered labels: [0]\n",
      "⚠️ Only triggered 1 out of 6 labels.\n"
     ]
    }
   ],
   "source": [
    "# 3. 遍历 WordNet 词汇\n",
    "seen_labels = set()\n",
    "max_words = 5000  # 你可以调小/调大以控制运行速度\n",
    "count = 0\n",
    "\n",
    "for synset in list(wn.all_synsets())[:max_words]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        # 构造简单句子\n",
    "        text = f\"{lemma}\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "            seen_labels.add(pred_label)\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count} samples; seen labels so far: {sorted(seen_labels)}\")\n",
    "        if len(seen_labels) == num_labels:\n",
    "            print(\"✅ All labels triggered!\")\n",
    "            break\n",
    "    if len(seen_labels) == num_labels:\n",
    "        break\n",
    "\n",
    "print(f\"Final triggered labels: {sorted(seen_labels)}\")\n",
    "if len(seen_labels) < num_labels:\n",
    "    print(f\"⚠️ Only triggered {len(seen_labels)} out of {num_labels} labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fb9414b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 samples; seen labels so far: [0]\n",
      "Processed 200 samples; seen labels so far: [0]\n",
      "Processed 300 samples; seen labels so far: [0]\n",
      "Processed 400 samples; seen labels so far: [0]\n",
      "Processed 500 samples; seen labels so far: [0]\n",
      "Processed 600 samples; seen labels so far: [0]\n",
      "Processed 700 samples; seen labels so far: [0]\n",
      "Processed 800 samples; seen labels so far: [0]\n",
      "Processed 900 samples; seen labels so far: [0]\n",
      "Processed 1000 samples; seen labels so far: [0]\n",
      "Processed 1100 samples; seen labels so far: [0]\n",
      "Processed 1200 samples; seen labels so far: [0]\n",
      "Processed 1300 samples; seen labels so far: [0]\n",
      "Processed 1400 samples; seen labels so far: [0]\n",
      "Processed 1500 samples; seen labels so far: [0]\n",
      "Processed 1600 samples; seen labels so far: [0]\n",
      "Processed 1700 samples; seen labels so far: [0]\n",
      "Processed 1800 samples; seen labels so far: [0]\n",
      "Processed 1900 samples; seen labels so far: [0]\n",
      "Processed 2000 samples; seen labels so far: [0]\n",
      "Processed 2100 samples; seen labels so far: [0]\n",
      "Processed 2200 samples; seen labels so far: [0]\n",
      "Processed 2300 samples; seen labels so far: [0]\n",
      "Processed 2400 samples; seen labels so far: [0]\n",
      "Processed 2500 samples; seen labels so far: [0]\n",
      "Processed 2600 samples; seen labels so far: [0]\n",
      "Processed 2700 samples; seen labels so far: [0]\n",
      "Processed 2800 samples; seen labels so far: [0]\n",
      "Processed 2900 samples; seen labels so far: [0]\n",
      "Processed 3000 samples; seen labels so far: [0]\n",
      "Processed 3100 samples; seen labels so far: [0]\n",
      "Processed 3200 samples; seen labels so far: [0]\n",
      "Processed 3300 samples; seen labels so far: [0]\n",
      "Processed 3400 samples; seen labels so far: [0]\n",
      "Processed 3500 samples; seen labels so far: [0]\n",
      "Processed 3600 samples; seen labels so far: [0]\n",
      "Processed 3700 samples; seen labels so far: [0]\n",
      "Processed 3800 samples; seen labels so far: [0]\n",
      "Processed 3900 samples; seen labels so far: [0]\n",
      "Processed 4000 samples; seen labels so far: [0]\n",
      "Processed 4100 samples; seen labels so far: [0]\n",
      "Processed 4200 samples; seen labels so far: [0]\n",
      "Processed 4300 samples; seen labels so far: [0]\n",
      "Processed 4400 samples; seen labels so far: [0]\n",
      "Processed 4500 samples; seen labels so far: [0]\n",
      "Processed 4600 samples; seen labels so far: [0]\n",
      "Processed 4700 samples; seen labels so far: [0]\n",
      "Processed 4800 samples; seen labels so far: [0]\n",
      "Processed 4900 samples; seen labels so far: [0]\n",
      "Processed 5000 samples; seen labels so far: [0]\n",
      "Processed 5100 samples; seen labels so far: [0]\n",
      "Processed 5200 samples; seen labels so far: [0]\n",
      "Processed 5300 samples; seen labels so far: [0]\n",
      "Processed 5400 samples; seen labels so far: [0]\n",
      "Processed 5500 samples; seen labels so far: [0]\n",
      "Processed 5600 samples; seen labels so far: [0]\n",
      "Processed 5700 samples; seen labels so far: [0]\n",
      "Processed 5800 samples; seen labels so far: [0]\n",
      "Processed 5900 samples; seen labels so far: [0]\n",
      "Processed 6000 samples; seen labels so far: [0]\n",
      "Processed 6100 samples; seen labels so far: [0]\n",
      "Processed 6200 samples; seen labels so far: [0]\n",
      "Processed 6300 samples; seen labels so far: [0]\n",
      "Processed 6400 samples; seen labels so far: [0]\n",
      "Processed 6500 samples; seen labels so far: [0]\n",
      "Processed 6600 samples; seen labels so far: [0]\n",
      "Processed 6700 samples; seen labels so far: [0]\n",
      "Processed 6800 samples; seen labels so far: [0]\n",
      "Processed 6900 samples; seen labels so far: [0]\n",
      "Processed 7000 samples; seen labels so far: [0]\n",
      "Processed 7100 samples; seen labels so far: [0]\n",
      "Processed 7200 samples; seen labels so far: [0]\n",
      "Processed 7300 samples; seen labels so far: [0]\n",
      "Processed 7400 samples; seen labels so far: [0]\n",
      "Processed 7500 samples; seen labels so far: [0]\n",
      "Processed 7600 samples; seen labels so far: [0]\n",
      "Processed 7700 samples; seen labels so far: [0]\n",
      "Processed 7800 samples; seen labels so far: [0]\n",
      "Processed 7900 samples; seen labels so far: [0]\n",
      "Processed 8000 samples; seen labels so far: [0]\n",
      "Processed 8100 samples; seen labels so far: [0]\n",
      "Processed 8200 samples; seen labels so far: [0]\n",
      "Processed 8300 samples; seen labels so far: [0]\n",
      "Processed 8400 samples; seen labels so far: [0]\n",
      "Processed 8500 samples; seen labels so far: [0]\n",
      "Processed 8600 samples; seen labels so far: [0]\n",
      "Final triggered labels: [0]\n",
      "⚠️ Only triggered 1 out of 6 labels.\n"
     ]
    }
   ],
   "source": [
    "# 3. 遍历 WordNet 词汇\n",
    "seen_labels = set()\n",
    "max_words = 5000  # 你可以调小/调大以控制运行速度\n",
    "count = 0\n",
    "\n",
    "for synset in list(wn.all_synsets())[:max_words]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        # 构造简单句子\n",
    "        text = f\"{lemma}{lemma}{lemma}{lemma}{lemma}{lemma}\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "            seen_labels.add(pred_label)\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count} samples; seen labels so far: {sorted(seen_labels)}\")\n",
    "        if len(seen_labels) == num_labels:\n",
    "            print(\"✅ All labels triggered!\")\n",
    "            break\n",
    "    if len(seen_labels) == num_labels:\n",
    "        break\n",
    "\n",
    "print(f\"Final triggered labels: {sorted(seen_labels)}\")\n",
    "if len(seen_labels) < num_labels:\n",
    "    print(f\"⚠️ Only triggered {len(seen_labels)} out of {num_labels} labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fefdd27",
   "metadata": {},
   "source": [
    "#### 模型五"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab182d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "\n",
    "# 1. 加载模型与 tokenizer\n",
    "model_name = \"nicholasKluge/ToxicityModel\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()\n",
    "# 2. 获取模型标签数\n",
    "num_labels = model.config.num_labels\n",
    "print(f\"Number of labels: {num_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6fdbe81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All labels triggered!\n",
      "Final triggered labels: [0]\n"
     ]
    }
   ],
   "source": [
    "# 3. 遍历 WordNet 词汇\n",
    "seen_labels = set()\n",
    "max_words = 5000  # 你可以调小/调大以控制运行速度\n",
    "count = 0\n",
    "\n",
    "for synset in list(wn.all_synsets())[:max_words]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        # 构造简单句子\n",
    "        text = f\"{lemma}\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "            seen_labels.add(pred_label)\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count} samples; seen labels so far: {sorted(seen_labels)}\")\n",
    "        if len(seen_labels) == num_labels:\n",
    "            print(\"✅ All labels triggered!\")\n",
    "            break\n",
    "    if len(seen_labels) == num_labels:\n",
    "        break\n",
    "\n",
    "print(f\"Final triggered labels: {sorted(seen_labels)}\")\n",
    "if len(seen_labels) < num_labels:\n",
    "    print(f\"⚠️ Only triggered {len(seen_labels)} out of {num_labels} labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f22bb40",
   "metadata": {},
   "source": [
    "# jailbreak-classifier\n",
    "    https://huggingface.co/jackhhao/jailbreak-classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0796eb",
   "metadata": {},
   "source": [
    "#### 模型1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63a489c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# 1. 加载模型与 tokenizer\n",
    "model_name = \"jackhhao/jailbreak-classifier\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()\n",
    "# 2. 获取模型标签数\n",
    "num_labels = model.config.num_labels\n",
    "print(f\"Number of labels: {num_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d45c98ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 samples; seen labels so far: [0]\n",
      "✅ All labels triggered!\n",
      "Final triggered labels: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "# 3. 遍历 WordNet 词汇\n",
    "seen_labels = set()\n",
    "max_words = 5000  # 你可以调小/调大以控制运行速度\n",
    "count = 0\n",
    "\n",
    "for synset in list(wn.all_synsets())[:max_words]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        # 构造简单句子\n",
    "        text = f\"{lemma}\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "            seen_labels.add(pred_label)\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count} samples; seen labels so far: {sorted(seen_labels)}\")\n",
    "        if len(seen_labels) == num_labels:\n",
    "            print(\"✅ All labels triggered!\")\n",
    "            break\n",
    "    if len(seen_labels) == num_labels:\n",
    "        break\n",
    "\n",
    "print(f\"Final triggered labels: {sorted(seen_labels)}\")\n",
    "if len(seen_labels) < num_labels:\n",
    "    print(f\"⚠️ Only triggered {len(seen_labels)} out of {num_labels} labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbd2ee3",
   "metadata": {},
   "source": [
    "#### 模型2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3b14816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# 1. 加载模型与 tokenizer\n",
    "model_name = \"lordofthejars/jailbreak-classifier\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()\n",
    "# 2. 获取模型标签数\n",
    "num_labels = model.config.num_labels\n",
    "print(f\"Number of labels: {num_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c0774cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 samples; seen labels so far: [0]\n",
      "✅ All labels triggered!\n",
      "Final triggered labels: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "seen_labels = set()\n",
    "max_words = 5000  \n",
    "count = 0\n",
    "\n",
    "for synset in list(wn.all_synsets())[:max_words]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        # 构造简单句子\n",
    "        text = f\"{lemma}\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "            seen_labels.add(pred_label)\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count} samples; seen labels so far: {sorted(seen_labels)}\")\n",
    "        if len(seen_labels) == num_labels:\n",
    "            print(\"✅ All labels triggered!\")\n",
    "            break\n",
    "    if len(seen_labels) == num_labels:\n",
    "        break\n",
    "\n",
    "print(f\"Final triggered labels: {sorted(seen_labels)}\")\n",
    "if len(seen_labels) < num_labels:\n",
    "    print(f\"⚠️ Only triggered {len(seen_labels)} out of {num_labels} labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e680d9",
   "metadata": {},
   "source": [
    "#### 模型三"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "720f072c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# 1. 加载模型与 tokenizer\n",
    "model_name = \"Necent/distilbert-base-uncased-detected-jailbreak\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()\n",
    "# 2. 获取模型标签数\n",
    "num_labels = model.config.num_labels\n",
    "print(f\"Number of labels: {num_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a39a0a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All labels triggered!\n",
      "Final triggered labels: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "seen_labels = set()\n",
    "max_words = 5000  \n",
    "count = 0\n",
    "\n",
    "for synset in list(wn.all_synsets())[:max_words]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        # 构造简单句子\n",
    "        text = f\"{lemma}\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "            seen_labels.add(pred_label)\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count} samples; seen labels so far: {sorted(seen_labels)}\")\n",
    "        if len(seen_labels) == num_labels:\n",
    "            print(\"✅ All labels triggered!\")\n",
    "            break\n",
    "    if len(seen_labels) == num_labels:\n",
    "        break\n",
    "\n",
    "print(f\"Final triggered labels: {sorted(seen_labels)}\")\n",
    "if len(seen_labels) < num_labels:\n",
    "    print(f\"⚠️ Only triggered {len(seen_labels)} out of {num_labels} labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5c8083",
   "metadata": {},
   "source": [
    "# Sarcasm-classifier\n",
    "    https://huggingface.co/hallisky/sarcasm-classifier-gpt4-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2bf9bd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# 1. 加载模型与 tokenizer\n",
    "model_name = \"hallisky/sarcasm-classifier-gpt4-data\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()\n",
    "# 2. 获取模型标签数\n",
    "num_labels = model.config.num_labels\n",
    "print(f\"Number of labels: {num_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "993143e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 samples; seen labels so far: [0]\n",
      "Processed 200 samples; seen labels so far: [0]\n",
      "Processed 300 samples; seen labels so far: [0]\n",
      "Processed 400 samples; seen labels so far: [0]\n",
      "Processed 500 samples; seen labels so far: [0]\n",
      "✅ All labels triggered!\n",
      "Final triggered labels: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "seen_labels = set()\n",
    "max_words = 5000  \n",
    "count = 0\n",
    "\n",
    "for synset in list(wn.all_synsets())[:max_words]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        # 构造简单句子\n",
    "        text = f\"{lemma}\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "            seen_labels.add(pred_label)\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count} samples; seen labels so far: {sorted(seen_labels)}\")\n",
    "        if len(seen_labels) == num_labels:\n",
    "            print(\"✅ All labels triggered!\")\n",
    "            break\n",
    "    if len(seen_labels) == num_labels:\n",
    "        break\n",
    "\n",
    "print(f\"Final triggered labels: {sorted(seen_labels)}\")\n",
    "if len(seen_labels) < num_labels:\n",
    "    print(f\"⚠️ Only triggered {len(seen_labels)} out of {num_labels} labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62be5fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "domain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
