{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6112f59",
   "metadata": {},
   "source": [
    "# 任务一：Emotion Classification\n",
    "   - 结论：对于 13 label，如果遍历wordnet所有input，只有Neutral这个标签。但如果input为“I feel {word}”,可得到12个label。 \n",
    "   - 测试第13个label 怀疑模型没训练好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98bf527f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 13\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# 1. 加载模型与 tokenizer\n",
    "model_name = \"boltuix/bert-emotion\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/ubuntu/dora/domain_inference/model/bert-emotion\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"/home/ubuntu/dora/domain_inference/model/bert-emotion\")\n",
    "model.eval()\n",
    "# 2. 获取模型标签数\n",
    "num_labels = model.config.num_labels\n",
    "print(f\"Number of labels: {num_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "babbcbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 samples; seen labels so far: [0, 1, 4, 5, 6, 7]\n",
      "Processed 200 samples; seen labels so far: [0, 1, 2, 4, 5, 6, 7, 8]\n",
      "Processed 300 samples; seen labels so far: [0, 1, 2, 4, 5, 6, 7, 8]\n",
      "Processed 400 samples; seen labels so far: [0, 1, 2, 4, 5, 6, 7, 8]\n",
      "Processed 500 samples; seen labels so far: [0, 1, 2, 4, 5, 6, 7, 8]\n",
      "Processed 600 samples; seen labels so far: [0, 1, 2, 4, 5, 6, 7, 8]\n",
      "Processed 700 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Processed 800 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Processed 900 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Processed 1000 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Processed 1100 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Processed 1200 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Processed 1300 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Processed 1400 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 1500 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 1600 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 1700 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 1800 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 1900 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 2000 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 2100 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 2200 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 2300 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 2400 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 2500 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 2600 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 2700 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 2800 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 2900 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 3000 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 3100 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 3200 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 3300 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 3400 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 3500 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 3600 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 3700 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 3800 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 3900 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 4000 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 4100 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 4200 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 4300 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 4400 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 4500 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 4600 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 4700 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 4800 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 4900 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 5000 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 5100 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 5200 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 5300 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 5400 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 5500 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 5600 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 5700 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 5800 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 5900 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 6000 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 6100 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 6200 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 6300 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 6400 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 6500 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 6600 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 6700 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 6800 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 6900 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 7000 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 7100 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 7200 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 7300 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 7400 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 7500 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 7600 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 7700 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 7800 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 7900 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 8000 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 8100 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 8200 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 8300 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 8400 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 8500 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 8600 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Final triggered labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "⚠️ Only triggered 11 out of 13 labels.\n"
     ]
    }
   ],
   "source": [
    "# 3. 遍历 WordNet 词汇（带prefix “I feel”）\n",
    "seen_labels = set()\n",
    "max_words = 5000  # 你可以调小/调大以控制运行速度\n",
    "count = 0\n",
    "\n",
    "for synset in list(wn.all_synsets())[:max_words]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        # 构造简单句子\n",
    "        text = f\"{lemma}\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "            seen_labels.add(pred_label)\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count} samples; seen labels so far: {sorted(seen_labels)}\")\n",
    "        if len(seen_labels) == num_labels:\n",
    "            print(\"✅ All labels triggered!\")\n",
    "            break\n",
    "    if len(seen_labels) == num_labels:\n",
    "        break\n",
    "\n",
    "print(f\"Final triggered labels: {sorted(seen_labels)}\")\n",
    "if len(seen_labels) < num_labels:\n",
    "    print(f\"⚠️ Only triggered {len(seen_labels)} out of {num_labels} labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58d92a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 samples; seen labels so far: [0, 1, 4, 5, 6, 7]\n",
      "Processed 200 samples; seen labels so far: [0, 1, 2, 4, 5, 6, 7, 8]\n",
      "Processed 300 samples; seen labels so far: [0, 1, 2, 4, 5, 6, 7, 8]\n",
      "Processed 400 samples; seen labels so far: [0, 1, 2, 4, 5, 6, 7, 8]\n",
      "Processed 500 samples; seen labels so far: [0, 1, 2, 4, 5, 6, 7, 8]\n",
      "Processed 600 samples; seen labels so far: [0, 1, 2, 4, 5, 6, 7, 8]\n",
      "Processed 700 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Processed 800 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Processed 900 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Processed 1000 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Processed 1100 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Processed 1200 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Processed 1300 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Processed 1400 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 1500 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 1600 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 1700 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 1800 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 1900 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 2000 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 2100 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 2200 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 2300 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 2400 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 2500 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 2600 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 2700 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 2800 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 2900 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 3000 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 3100 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 3200 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 3300 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 3400 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 3500 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 3600 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 3700 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 3800 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 3900 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 4000 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 4100 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 4200 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processed 4300 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 4400 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 4500 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 4600 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 4700 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 4800 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 4900 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 5000 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 5100 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 5200 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 5300 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 5400 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 5500 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 5600 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 5700 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 5800 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 5900 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 6000 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 6100 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 6200 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 6300 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 6400 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 6500 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 6600 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 6700 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 6800 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 6900 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 7000 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 7100 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 7200 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 7300 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 7400 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 7500 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 7600 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 7700 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 7800 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 7900 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 8000 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 8100 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 8200 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 8300 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 8400 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 8500 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Processed 8600 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Final triggered labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "⚠️ Only triggered 11 out of 13 labels.\n"
     ]
    }
   ],
   "source": [
    "## 加空格\n",
    "seen_labels = set()\n",
    "max_words = 5000  \n",
    "count = 0\n",
    "\n",
    "for synset in list(wn.all_synsets())[:max_words]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        # 构造简单句子\n",
    "        text = f\"  {lemma}\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "            seen_labels.add(pred_label)\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count} samples; seen labels so far: {sorted(seen_labels)}\")\n",
    "        if len(seen_labels) == num_labels:\n",
    "            print(\"✅ All labels triggered!\")\n",
    "            break\n",
    "    if len(seen_labels) == num_labels:\n",
    "        break\n",
    "\n",
    "print(f\"Final triggered labels: {sorted(seen_labels)}\")\n",
    "if len(seen_labels) < num_labels:\n",
    "    print(f\"⚠️ Only triggered {len(seen_labels)} out of {num_labels} labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad4dcb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 samples; seen labels so far: [0, 1, 2, 4, 5, 6, 7, 8]\n",
      "Processed 200 samples; seen labels so far: [0, 1, 2, 4, 5, 6, 7, 8]\n",
      "Processed 300 samples; seen labels so far: [0, 1, 2, 4, 5, 6, 7, 8]\n",
      "Processed 400 samples; seen labels so far: [0, 1, 2, 4, 5, 6, 7, 8]\n",
      "Processed 500 samples; seen labels so far: [0, 1, 2, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 600 samples; seen labels so far: [0, 1, 2, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 700 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 800 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 900 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 1000 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 1100 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 1200 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 1300 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 1400 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 1500 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 1600 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 1700 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 1800 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 1900 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 2000 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 2100 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 2200 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 2300 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 2400 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 2500 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 2600 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 2700 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 2800 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 2900 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 3000 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 3100 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 3200 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 3300 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 3400 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 3500 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 3600 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 3700 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 3800 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 3900 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 4000 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 4100 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 4200 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Processed 4300 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 4400 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 4500 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 4600 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 4700 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 4800 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 4900 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 5000 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 5100 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 5200 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 5300 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 5400 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 5500 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 5600 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 5700 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 5800 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 5900 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 6000 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 6100 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 6200 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 6300 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 6400 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 6500 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 6600 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 6700 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 6800 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 6900 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 7000 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 7100 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 7200 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 7300 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 7400 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 7500 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 7600 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 7700 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 7800 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 7900 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 8000 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 8100 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 8200 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 8300 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 8400 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 8500 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Processed 8600 samples; seen labels so far: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Final triggered labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "⚠️ Only triggered 12 out of 13 labels.\n"
     ]
    }
   ],
   "source": [
    "## 重复\n",
    "seen_labels = set()\n",
    "max_words = 5000  \n",
    "count = 0\n",
    "\n",
    "for synset in list(wn.all_synsets())[:max_words]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        # 构造简单句子\n",
    "        text = f\"{lemma} {lemma} {lemma}\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "            seen_labels.add(pred_label)\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count} samples; seen labels so far: {sorted(seen_labels)}\")\n",
    "        if len(seen_labels) == num_labels:\n",
    "            print(\"✅ All labels triggered!\")\n",
    "            break\n",
    "    if len(seen_labels) == num_labels:\n",
    "        break\n",
    "\n",
    "print(f\"Final triggered labels: {sorted(seen_labels)}\")\n",
    "if len(seen_labels) < num_labels:\n",
    "    print(f\"⚠️ Only triggered {len(seen_labels)} out of {num_labels} labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99512fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "#https://huggingface.co/boltuix/bert-emotion\n",
    "text = f\"happy\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "064629f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# Sarcasm 是第12个label 的ground truth，但是不足以触发\n",
    "text = f\"Oh sure, like that's gonna work.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7521ed65",
   "metadata": {},
   "source": [
    "# 任务二：Sentiment Classification\n",
    " - prefix必须要'I feel'->五个标签都触发了 \n",
    "https://www.modelscope.cn/models/Xenova/bert-base-multilingual-uncased-sentiment/files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad8c5e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Input Names: ['input_ids', 'attention_mask', 'token_type_ids']\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "# 使用 HuggingFace 模型（等效于 Xenova ONNX 模型的原始版本）\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "model_path = \"/home/ubuntu/dora/domain_inference/model/sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = ort.InferenceSession(f\"{model_path}/onnx/model.onnx\")\n",
    "# model.eval()\n",
    "\n",
    "input_names = [inp.name for inp in model.get_inputs()]\n",
    "print(\"ONNX Input Names:\", input_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ab43f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I feel able.\n",
      "I feel unable.\n",
      "I feel abaxial.\n",
      "I feel dorsal.\n",
      "I feel adaxial.\n",
      "I feel ventral.\n",
      "I feel acroscopic.\n",
      "I feel basiscopic.\n",
      "I feel abducent.\n",
      "I feel abducting.\n",
      "I feel adducent.\n",
      "I feel adductive.\n",
      "I feel adducting.\n",
      "I feel nascent.\n",
      "I feel emergent.\n",
      "I feel emerging.\n",
      "I feel dissilient.\n",
      "I feel parturient.\n",
      "I feel dying.\n",
      "I feel moribund.\n",
      "I feel last.\n",
      "I feel abridged.\n",
      "I feel cut.\n",
      "I feel shortened.\n",
      "I feel half-length.\n",
      "I feel potted.\n",
      "I feel unabridged.\n",
      "I feel full-length.\n",
      "I feel uncut.\n",
      "I feel absolute.\n",
      "I feel direct.\n",
      "I feel implicit.\n",
      "I feel unquestioning.\n",
      "I feel infinite.\n",
      "I feel living.\n",
      "I feel relative.\n",
      "I feel comparative.\n",
      "I feel relational.\n",
      "✅ All 5 labels triggered\n",
      "最终触发标签: [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 情感标签：5类\n",
    "seen_labels = set()\n",
    "max_synsets = 5000\n",
    "count = 0\n",
    "\n",
    "for synset in list(wn.all_synsets())[:max_synsets]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        text = f\"I feel {lemma}.\"\n",
    "        print(text)\n",
    "        enc = tokenizer(text, return_tensors=\"np\", truncation=True)\n",
    "        \n",
    "        # ONNX 推理\n",
    "        ort_inputs = {name: enc[name] for name in input_names}\n",
    "        ort_outs = model.run(None, ort_inputs)\n",
    "        \n",
    "        logits = ort_outs[0]\n",
    "        pred = int(np.argmax(logits, axis=1)[0])  # 0~4\n",
    "        seen_labels.add(pred)\n",
    "        \n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count}; Seen labels: {sorted(seen_labels)}\")\n",
    "        if len(seen_labels) == 5:\n",
    "            print(\"✅ All 5 labels triggered\")\n",
    "            break\n",
    "    if len(seen_labels) == 5:\n",
    "        break\n",
    "\n",
    "print(f\"最终触发标签: {sorted(seen_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f73f57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All 5 labels triggered\n",
      "最终触发标签: [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# 情感标签：5类\n",
    "seen_labels = set()\n",
    "max_synsets = 5000\n",
    "count = 0\n",
    "\n",
    "for synset in list(wn.all_synsets())[:max_synsets]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        text = f\"{lemma}.\"\n",
    "        # print(text)\n",
    "        enc = tokenizer(text, return_tensors=\"np\", truncation=True)\n",
    "        \n",
    "        # ONNX 推理\n",
    "        ort_inputs = {name: enc[name] for name in input_names}\n",
    "        ort_outs = model.run(None, ort_inputs)\n",
    "        \n",
    "        logits = ort_outs[0]\n",
    "        pred = int(np.argmax(logits, axis=1)[0])  # 0~4\n",
    "        seen_labels.add(pred)\n",
    "        \n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count}; Seen labels: {sorted(seen_labels)}\")\n",
    "        if len(seen_labels) == 5:\n",
    "            print(\"✅ All 5 labels triggered\")\n",
    "            break\n",
    "    if len(seen_labels) == 5:\n",
    "        break\n",
    "\n",
    "print(f\"最终触发标签: {sorted(seen_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f8ad62",
   "metadata": {},
   "source": [
    "# 任务三：Toxicity Classification \n",
    " - 这个模型输出的是6个标签的logits\n",
    " - prefix 为 “You are such a” 6个标签触发5个\n",
    " - https://www.modelscope.cn/models/Xenova/toxic-bert/files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5cae108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型输出类别数: 6\n",
      "ONNX Input Names: ['input_ids', 'attention_mask', 'token_type_ids']\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "# 使用 HuggingFace 模型（等效于 Xenova ONNX 模型的原始版本）\n",
    "model_name = \"Xenova/toxic-bert\"\n",
    "model_path = \"/home/ubuntu/dora/domain_inference/model/toxic\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = ort.InferenceSession(f\"{model_path}/onnx/model.onnx\")\n",
    "# model.eval()\n",
    "\n",
    "num_classes = model.get_outputs()[0].shape[1]\n",
    "print(f\"模型输出类别数: {num_classes}\")\n",
    "input_names = [inp.name for inp in model.get_inputs()]\n",
    "print(\"ONNX Input Names:\", input_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9cf310c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100; Triggered labels so far: ['insult', 'obscene', 'toxic']\n",
      "Processed 200; Triggered labels so far: ['insult', 'obscene', 'toxic']\n",
      "Processed 300; Triggered labels so far: ['insult', 'obscene', 'toxic']\n",
      "Processed 400; Triggered labels so far: ['insult', 'obscene', 'toxic']\n",
      "Processed 500; Triggered labels so far: ['insult', 'obscene', 'toxic']\n",
      "Processed 600; Triggered labels so far: ['insult', 'obscene', 'toxic']\n",
      "Processed 700; Triggered labels so far: ['insult', 'obscene', 'toxic']\n",
      "Processed 800; Triggered labels so far: ['insult', 'obscene', 'toxic']\n",
      "Processed 900; Triggered labels so far: ['insult', 'obscene', 'toxic']\n",
      "Processed 1000; Triggered labels so far: ['insult', 'obscene', 'toxic']\n",
      "Processed 1100; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 1200; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 1300; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 1400; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 1500; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 1600; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 1700; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 1800; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 1900; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 2000; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 2100; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 2200; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 2300; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 2400; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 2500; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 2600; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 2700; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 2800; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 2900; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 3000; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 3100; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 3200; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 3300; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 3400; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 3500; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 3600; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 3700; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 3800; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 3900; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 4000; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 4100; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 4200; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 4300; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 4400; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 4500; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'toxic']\n",
      "Processed 4600; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 4700; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 4800; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 4900; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 5000; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 5100; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 5200; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 5300; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 5400; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 5500; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 5600; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 5700; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 5800; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 5900; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 6000; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 6100; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 6200; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 6300; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 6400; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 6500; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 6600; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 6700; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 6800; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 6900; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 7000; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 7100; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 7200; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 7300; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 7400; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 7500; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 7600; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 7700; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 7800; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 7900; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 8000; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 8100; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 8200; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 8300; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 8400; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 8500; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 8600; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "最终触发标签: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "⚠️ 仅触发了 5 / 6 个标签\n"
     ]
    }
   ],
   "source": [
    "# 毒性标签\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "seen_labels = set()\n",
    "num_labels = len(labels)\n",
    "threshold = 0.5\n",
    "count = 0\n",
    "max_synsets = 5000\n",
    "\n",
    "# 遍历 WordNet\n",
    "for synset in list(wn.all_synsets())[:max_synsets]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        text = f\"You are such a {lemma}.\"\n",
    "        encoded = tokenizer(text, return_tensors=\"np\", truncation=True, padding='max_length', max_length=128)\n",
    "        \n",
    "        # 提取模型输入（如 input_ids, attention_mask）\n",
    "        ort_inputs = {name: encoded[name] for name in input_names}\n",
    "        ort_outs = model.run(None, ort_inputs)\n",
    "        \n",
    "        logits = ort_outs[0]  # shape [1, 6]\n",
    "        probs = 1 / (1 + np.exp(-logits[0]))  # sigmoid manually\n",
    "\n",
    "        for i, prob in enumerate(probs):\n",
    "            if prob >= threshold:\n",
    "                seen_labels.add(labels[i])\n",
    "\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count}; Triggered labels so far: {sorted(seen_labels)}\")\n",
    "        if len(seen_labels) == num_labels:\n",
    "            print(\"✅ All labels triggered\")\n",
    "            break\n",
    "    if len(seen_labels) == num_labels:\n",
    "        break\n",
    "\n",
    "print(f\"最终触发标签: {sorted(seen_labels)}\")\n",
    "if len(seen_labels) < num_labels:\n",
    "    print(f\"⚠️ 仅触发了 {len(seen_labels)} / {num_labels} 个标签\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4d7eb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100; Triggered labels so far: ['obscene', 'threat', 'toxic']\n",
      "Processed 200; Triggered labels so far: ['obscene', 'threat', 'toxic']\n",
      "Processed 300; Triggered labels so far: ['obscene', 'threat', 'toxic']\n",
      "Processed 400; Triggered labels so far: ['obscene', 'threat', 'toxic']\n",
      "Processed 500; Triggered labels so far: ['obscene', 'threat', 'toxic']\n",
      "Processed 600; Triggered labels so far: ['obscene', 'threat', 'toxic']\n",
      "Processed 700; Triggered labels so far: ['obscene', 'threat', 'toxic']\n",
      "Processed 800; Triggered labels so far: ['obscene', 'threat', 'toxic']\n",
      "Processed 900; Triggered labels so far: ['obscene', 'threat', 'toxic']\n",
      "Processed 1000; Triggered labels so far: ['obscene', 'threat', 'toxic']\n",
      "Processed 1100; Triggered labels so far: ['obscene', 'threat', 'toxic']\n",
      "Processed 1200; Triggered labels so far: ['obscene', 'threat', 'toxic']\n",
      "Processed 1300; Triggered labels so far: ['obscene', 'threat', 'toxic']\n",
      "Processed 1400; Triggered labels so far: ['insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 1500; Triggered labels so far: ['insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 1600; Triggered labels so far: ['insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 1700; Triggered labels so far: ['insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 1800; Triggered labels so far: ['insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 1900; Triggered labels so far: ['insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 2000; Triggered labels so far: ['insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 2100; Triggered labels so far: ['insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 2200; Triggered labels so far: ['insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 2300; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 2400; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 2500; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 2600; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 2700; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 2800; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 2900; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 3000; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 3100; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 3200; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 3300; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 3400; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 3500; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 3600; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 3700; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 3800; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 3900; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 4000; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 4100; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 4200; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 4300; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 4400; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 4500; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 4600; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 4700; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 4800; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 4900; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 5000; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 5100; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 5200; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 5300; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 5400; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 5500; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 5600; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 5700; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 5800; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 5900; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 6000; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 6100; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 6200; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 6300; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 6400; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 6500; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 6600; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 6700; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 6800; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 6900; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 7000; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 7100; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 7200; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 7300; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 7400; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n",
      "Processed 7500; Triggered labels so far: ['identity_hate', 'insult', 'obscene', 'threat', 'toxic']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# 提取模型输入（如 input_ids, attention_mask）\u001b[39;00m\n\u001b[1;32m     17\u001b[0m ort_inputs \u001b[38;5;241m=\u001b[39m {name: encoded[name] \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m input_names}\n\u001b[0;32m---> 18\u001b[0m ort_outs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mort_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m logits \u001b[38;5;241m=\u001b[39m ort_outs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# shape [1, 6]\u001b[39;00m\n\u001b[1;32m     21\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mlogits[\u001b[38;5;241m0\u001b[39m]))  \u001b[38;5;66;03m# sigmoid manually\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bullying/lib/python3.11/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:273\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    271\u001b[0m     output_names \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_meta]\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m C\u001b[38;5;241m.\u001b[39mEPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 毒性标签\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "seen_labels = set()\n",
    "num_labels = len(labels)\n",
    "threshold = 0.2\n",
    "count = 0\n",
    "max_synsets = 5000\n",
    "\n",
    "# 遍历 WordNet\n",
    "for synset in list(wn.all_synsets())[:max_synsets]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        text = f\"   {lemma}.\"\n",
    "        # print(text)\n",
    "        encoded = tokenizer(text, return_tensors=\"np\", truncation=True, padding='max_length', max_length=128)\n",
    "        \n",
    "        # 提取模型输入（如 input_ids, attention_mask）\n",
    "        ort_inputs = {name: encoded[name] for name in input_names}\n",
    "        ort_outs = model.run(None, ort_inputs)\n",
    "        \n",
    "        logits = ort_outs[0]  # shape [1, 6]\n",
    "        probs = 1 / (1 + np.exp(-logits[0]))  # sigmoid manually\n",
    "\n",
    "        for i, prob in enumerate(probs):\n",
    "            if prob >= threshold:\n",
    "                seen_labels.add(labels[i])\n",
    "\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count}; Triggered labels so far: {sorted(seen_labels)}\")\n",
    "        if len(seen_labels) == num_labels:\n",
    "            print(\"✅ All labels triggered\")\n",
    "            break\n",
    "    if len(seen_labels) == num_labels:\n",
    "        break\n",
    "\n",
    "print(f\"最终触发标签: {sorted(seen_labels)}\")\n",
    "if len(seen_labels) < num_labels:\n",
    "    print(f\"⚠️ 仅触发了 {len(seen_labels)} / {num_labels} 个标签\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2956ec5",
   "metadata": {},
   "source": [
    "# 任务四：Spam Classification \n",
    " - 2个标签触发1个，不能找到spam\n",
    " - https://www.modelscope.cn/models/onnx-community/bert-tiny-finetuned-sms-spam-detection-ONNX/files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b3592b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型输出类别数: 2\n",
      "ONNX Input Names: ['input_ids', 'attention_mask', 'token_type_ids']\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "# 使用 HuggingFace 模型（等效于 Xenova ONNX 模型的原始版本）\n",
    "model_name = \"onnx-community/bert-tiny-finetuned-sms-spam-detection-ONNX\"\n",
    "model_path = \"/home/ubuntu/dora/domain_inference/model/spam\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = ort.InferenceSession(f\"{model_path}/onnx/model.onnx\")\n",
    "# model.eval()\n",
    "\n",
    "num_classes = model.get_outputs()[0].shape[1]\n",
    "print(f\"模型输出类别数: {num_classes}\")\n",
    "input_names = [inp.name for inp in model.get_inputs()]\n",
    "print(\"ONNX Input Names:\", input_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ac8527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100；当前触发标签: ['ham']\n",
      "Processed 200；当前触发标签: ['ham']\n",
      "Processed 300；当前触发标签: ['ham']\n",
      "Processed 400；当前触发标签: ['ham']\n",
      "Processed 500；当前触发标签: ['ham']\n",
      "Processed 600；当前触发标签: ['ham']\n",
      "Processed 700；当前触发标签: ['ham']\n",
      "Processed 800；当前触发标签: ['ham']\n",
      "Processed 900；当前触发标签: ['ham']\n",
      "Processed 1000；当前触发标签: ['ham']\n",
      "Processed 1100；当前触发标签: ['ham']\n",
      "Processed 1200；当前触发标签: ['ham']\n",
      "Processed 1300；当前触发标签: ['ham']\n",
      "Processed 1400；当前触发标签: ['ham']\n",
      "Processed 1500；当前触发标签: ['ham']\n",
      "Processed 1600；当前触发标签: ['ham']\n",
      "Processed 1700；当前触发标签: ['ham']\n",
      "Processed 1800；当前触发标签: ['ham']\n",
      "Processed 1900；当前触发标签: ['ham']\n",
      "Processed 2000；当前触发标签: ['ham']\n",
      "Processed 2100；当前触发标签: ['ham']\n",
      "Processed 2200；当前触发标签: ['ham']\n",
      "Processed 2300；当前触发标签: ['ham']\n",
      "Processed 2400；当前触发标签: ['ham']\n",
      "Processed 2500；当前触发标签: ['ham']\n",
      "Processed 2600；当前触发标签: ['ham']\n",
      "Processed 2700；当前触发标签: ['ham']\n",
      "Processed 2800；当前触发标签: ['ham']\n",
      "Processed 2900；当前触发标签: ['ham']\n",
      "Processed 3000；当前触发标签: ['ham']\n",
      "Processed 3100；当前触发标签: ['ham']\n",
      "Processed 3200；当前触发标签: ['ham']\n",
      "Processed 3300；当前触发标签: ['ham']\n",
      "Processed 3400；当前触发标签: ['ham']\n",
      "Processed 3500；当前触发标签: ['ham']\n",
      "Processed 3600；当前触发标签: ['ham']\n",
      "Processed 3700；当前触发标签: ['ham']\n",
      "Processed 3800；当前触发标签: ['ham']\n",
      "Processed 3900；当前触发标签: ['ham']\n",
      "Processed 4000；当前触发标签: ['ham']\n",
      "Processed 4100；当前触发标签: ['ham']\n",
      "Processed 4200；当前触发标签: ['ham']\n",
      "Processed 4300；当前触发标签: ['ham']\n",
      "Processed 4400；当前触发标签: ['ham']\n",
      "Processed 4500；当前触发标签: ['ham']\n",
      "Processed 4600；当前触发标签: ['ham']\n",
      "Processed 4700；当前触发标签: ['ham']\n",
      "Processed 4800；当前触发标签: ['ham']\n",
      "Processed 4900；当前触发标签: ['ham']\n",
      "Processed 5000；当前触发标签: ['ham']\n",
      "Processed 5100；当前触发标签: ['ham']\n",
      "Processed 5200；当前触发标签: ['ham']\n",
      "Processed 5300；当前触发标签: ['ham']\n",
      "Processed 5400；当前触发标签: ['ham']\n",
      "Processed 5500；当前触发标签: ['ham']\n",
      "Processed 5600；当前触发标签: ['ham']\n",
      "Processed 5700；当前触发标签: ['ham']\n",
      "Processed 5800；当前触发标签: ['ham']\n",
      "Processed 5900；当前触发标签: ['ham']\n",
      "Processed 6000；当前触发标签: ['ham']\n",
      "Processed 6100；当前触发标签: ['ham']\n",
      "Processed 6200；当前触发标签: ['ham']\n",
      "Processed 6300；当前触发标签: ['ham']\n",
      "Processed 6400；当前触发标签: ['ham']\n",
      "Processed 6500；当前触发标签: ['ham']\n",
      "Processed 6600；当前触发标签: ['ham']\n",
      "Processed 6700；当前触发标签: ['ham']\n",
      "Processed 6800；当前触发标签: ['ham']\n",
      "Processed 6900；当前触发标签: ['ham']\n",
      "Processed 7000；当前触发标签: ['ham']\n",
      "Processed 7100；当前触发标签: ['ham']\n",
      "Processed 7200；当前触发标签: ['ham']\n",
      "Processed 7300；当前触发标签: ['ham']\n",
      "Processed 7400；当前触发标签: ['ham']\n",
      "Processed 7500；当前触发标签: ['ham']\n",
      "Processed 7600；当前触发标签: ['ham']\n",
      "Processed 7700；当前触发标签: ['ham']\n",
      "Processed 7800；当前触发标签: ['ham']\n",
      "Processed 7900；当前触发标签: ['ham']\n",
      "Processed 8000；当前触发标签: ['ham']\n",
      "Processed 8100；当前触发标签: ['ham']\n",
      "Processed 8200；当前触发标签: ['ham']\n",
      "Processed 8300；当前触发标签: ['ham']\n",
      "Processed 8400；当前触发标签: ['ham']\n",
      "Processed 8500；当前触发标签: ['ham']\n",
      "Processed 8600；当前触发标签: ['ham']\n",
      "最终触发标签: ['ham']\n"
     ]
    }
   ],
   "source": [
    "id2label = {0: \"ham\", 1: \"spam\"}\n",
    "\n",
    "# 遍历 WordNet\n",
    "max_synsets = 5000\n",
    "seen_labels = set()\n",
    "count = 0\n",
    "\n",
    "for synset in list(wn.all_synsets())[:max_synsets]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        # 构造类似短信的输入\n",
    "        text = f\"lemma}.\"\n",
    "        enc = tokenizer(text, return_tensors=\"np\", padding=\"max_length\", max_length=128, truncation=True)\n",
    "        \n",
    "        # ONNX 输入推理\n",
    "        ort_inputs = {name: enc[name] for name in input_names}\n",
    "        ort_outs = model.run(None, ort_inputs)\n",
    "        \n",
    "        logits = ort_outs[0]  # shape: [1, 2]\n",
    "        pred = int(np.argmax(logits, axis=1)[0])\n",
    "        seen_labels.add(pred)\n",
    "\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count}；当前触发标签: {[id2label[i] for i in sorted(seen_labels)]}\")\n",
    "        if len(seen_labels) == 2:\n",
    "            print(\"✅ 已触发 ham 和 spam 两个标签\")\n",
    "            break\n",
    "    if len(seen_labels) == 2:\n",
    "        break\n",
    "\n",
    "print(f\"最终触发标签: {[id2label[i] for i in sorted(seen_labels)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d01f3f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcbe0b42",
   "metadata": {},
   "source": [
    "# 情感分类任务\n",
    "    cardiffnlp/twitter-roberta-base-sentiment-latest\n",
    "    https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c77817a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1548a005",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# 1. 加载模型与 tokenizer\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()\n",
    "# 2. 获取模型标签数\n",
    "num_labels = model.config.num_labels\n",
    "print(f\"Number of labels: {num_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8132c533",
   "metadata": {},
   "source": [
    "### 只用wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c04ad161",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 samples; seen labels so far: [0, 1]\n",
      "✅ All labels triggered!\n",
      "Final triggered labels: [0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "seen_labels = set()\n",
    "max_words = 5000  \n",
    "count = 0\n",
    "\n",
    "for synset in list(wn.all_synsets())[:max_words]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        # 构造简单句子\n",
    "        text = f\"{lemma}\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "            seen_labels.add(pred_label)\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count} samples; seen labels so far: {sorted(seen_labels)}\")\n",
    "        if len(seen_labels) == num_labels:\n",
    "            print(\"✅ All labels triggered!\")\n",
    "            break\n",
    "    if len(seen_labels) == num_labels:\n",
    "        break\n",
    "\n",
    "print(f\"Final triggered labels: {sorted(seen_labels)}\")\n",
    "if len(seen_labels) < num_labels:\n",
    "    print(f\"⚠️ Only triggered {len(seen_labels)} out of {num_labels} labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f017c280",
   "metadata": {},
   "source": [
    "# Toxicity_classifier\n",
    "s-nlp/roberta_toxicity_classifier\n",
    "https://huggingface.co/s-nlp/roberta_toxicity_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "904c7453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of the model checkpoint at s-nlp/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('s-nlp/roberta_toxicity_classifier')\n",
    "model = RobertaForSequenceClassification.from_pretrained('s-nlp/roberta_toxicity_classifier')\n",
    "# 2. 获取模型标签数\n",
    "num_labels = model.config.num_labels\n",
    "print(f\"Number of labels: {num_labels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffc805d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All labels triggered!\n",
      "Final triggered labels: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "seen_labels = set()\n",
    "max_words = 5000  \n",
    "count = 0\n",
    "\n",
    "for synset in list(wn.all_synsets())[:max_words]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        # 构造简单句子\n",
    "        text = f\"{lemma}\"\n",
    "        inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            pred_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "            seen_labels.add(pred_label)\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count} samples; seen labels so far: {sorted(seen_labels)}\")\n",
    "        if len(seen_labels) == num_labels:\n",
    "            print(\"✅ All labels triggered!\")\n",
    "            break\n",
    "    if len(seen_labels) == num_labels:\n",
    "        break\n",
    "\n",
    "print(f\"Final triggered labels: {sorted(seen_labels)}\")\n",
    "if len(seen_labels) < num_labels:\n",
    "    print(f\"⚠️ Only triggered {len(seen_labels)} out of {num_labels} labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f22bb40",
   "metadata": {},
   "source": [
    "# jailbreak-classifier\n",
    "    https://huggingface.co/jackhhao/jailbreak-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63a489c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# 1. 加载模型与 tokenizer\n",
    "model_name = \"jackhhao/jailbreak-classifier\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()\n",
    "# 2. 获取模型标签数\n",
    "num_labels = model.config.num_labels\n",
    "print(f\"Number of labels: {num_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbd2ee3",
   "metadata": {},
   "source": [
    "## 只用wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c0774cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 samples; seen labels so far: [0]\n",
      "✅ All labels triggered!\n",
      "Final triggered labels: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "seen_labels = set()\n",
    "max_words = 5000  \n",
    "count = 0\n",
    "\n",
    "for synset in list(wn.all_synsets())[:max_words]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        # 构造简单句子\n",
    "        text = f\"{lemma}\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "            seen_labels.add(pred_label)\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count} samples; seen labels so far: {sorted(seen_labels)}\")\n",
    "        if len(seen_labels) == num_labels:\n",
    "            print(\"✅ All labels triggered!\")\n",
    "            break\n",
    "    if len(seen_labels) == num_labels:\n",
    "        break\n",
    "\n",
    "print(f\"Final triggered labels: {sorted(seen_labels)}\")\n",
    "if len(seen_labels) < num_labels:\n",
    "    print(f\"⚠️ Only triggered {len(seen_labels)} out of {num_labels} labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5093ca18",
   "metadata": {},
   "source": [
    "# spam-classifier\n",
    "    https://huggingface.co/mrm8488/bert-tiny-finetuned-sms-spam-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a90080ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# 1. 加载模型与 tokenizer\n",
    "model_name = \"mrm8488/bert-tiny-finetuned-sms-spam-detection\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()\n",
    "# 2. 获取模型标签数\n",
    "num_labels = model.config.num_labels\n",
    "print(f\"Number of labels: {num_labels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d2eb5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 samples; seen labels so far: [0]\n",
      "Processed 200 samples; seen labels so far: [0]\n",
      "Processed 300 samples; seen labels so far: [0]\n",
      "Processed 400 samples; seen labels so far: [0]\n",
      "Processed 500 samples; seen labels so far: [0]\n",
      "Processed 600 samples; seen labels so far: [0]\n",
      "Processed 700 samples; seen labels so far: [0]\n",
      "Processed 800 samples; seen labels so far: [0]\n",
      "Processed 900 samples; seen labels so far: [0]\n",
      "Processed 1000 samples; seen labels so far: [0]\n",
      "Processed 1100 samples; seen labels so far: [0]\n",
      "Processed 1200 samples; seen labels so far: [0]\n",
      "Processed 1300 samples; seen labels so far: [0]\n",
      "Processed 1400 samples; seen labels so far: [0]\n",
      "Processed 1500 samples; seen labels so far: [0]\n",
      "Processed 1600 samples; seen labels so far: [0]\n",
      "Processed 1700 samples; seen labels so far: [0]\n",
      "Processed 1800 samples; seen labels so far: [0]\n",
      "Processed 1900 samples; seen labels so far: [0]\n",
      "Processed 2000 samples; seen labels so far: [0]\n",
      "Processed 2100 samples; seen labels so far: [0]\n",
      "Processed 2200 samples; seen labels so far: [0]\n",
      "Processed 2300 samples; seen labels so far: [0]\n",
      "Processed 2400 samples; seen labels so far: [0]\n",
      "Processed 2500 samples; seen labels so far: [0]\n",
      "Processed 2600 samples; seen labels so far: [0]\n",
      "Processed 2700 samples; seen labels so far: [0]\n",
      "Processed 2800 samples; seen labels so far: [0]\n",
      "Processed 2900 samples; seen labels so far: [0]\n",
      "Processed 3000 samples; seen labels so far: [0]\n",
      "Processed 3100 samples; seen labels so far: [0]\n",
      "Processed 3200 samples; seen labels so far: [0]\n",
      "Processed 3300 samples; seen labels so far: [0]\n",
      "Processed 3400 samples; seen labels so far: [0]\n",
      "Processed 3500 samples; seen labels so far: [0]\n",
      "Processed 3600 samples; seen labels so far: [0]\n",
      "Processed 3700 samples; seen labels so far: [0]\n",
      "Processed 3800 samples; seen labels so far: [0]\n",
      "Processed 3900 samples; seen labels so far: [0]\n",
      "Processed 4000 samples; seen labels so far: [0]\n",
      "Processed 4100 samples; seen labels so far: [0]\n",
      "Processed 4200 samples; seen labels so far: [0]\n",
      "Processed 4300 samples; seen labels so far: [0]\n",
      "Processed 4400 samples; seen labels so far: [0]\n",
      "Processed 4500 samples; seen labels so far: [0]\n",
      "Processed 4600 samples; seen labels so far: [0]\n",
      "Processed 4700 samples; seen labels so far: [0]\n",
      "Processed 4800 samples; seen labels so far: [0]\n",
      "Processed 4900 samples; seen labels so far: [0]\n",
      "Processed 5000 samples; seen labels so far: [0]\n",
      "Processed 5100 samples; seen labels so far: [0]\n",
      "Processed 5200 samples; seen labels so far: [0]\n",
      "Processed 5300 samples; seen labels so far: [0]\n",
      "Processed 5400 samples; seen labels so far: [0]\n",
      "Processed 5500 samples; seen labels so far: [0]\n",
      "Processed 5600 samples; seen labels so far: [0]\n",
      "Processed 5700 samples; seen labels so far: [0]\n",
      "Processed 5800 samples; seen labels so far: [0]\n",
      "Processed 5900 samples; seen labels so far: [0]\n",
      "Processed 6000 samples; seen labels so far: [0]\n",
      "Processed 6100 samples; seen labels so far: [0]\n",
      "Processed 6200 samples; seen labels so far: [0]\n",
      "Processed 6300 samples; seen labels so far: [0]\n",
      "Processed 6400 samples; seen labels so far: [0]\n",
      "Processed 6500 samples; seen labels so far: [0]\n",
      "Processed 6600 samples; seen labels so far: [0]\n",
      "Processed 6700 samples; seen labels so far: [0]\n",
      "Processed 6800 samples; seen labels so far: [0]\n",
      "Processed 6900 samples; seen labels so far: [0]\n",
      "Processed 7000 samples; seen labels so far: [0]\n",
      "Processed 7100 samples; seen labels so far: [0]\n",
      "Processed 7200 samples; seen labels so far: [0]\n",
      "Processed 7300 samples; seen labels so far: [0]\n",
      "Processed 7400 samples; seen labels so far: [0]\n",
      "Processed 7500 samples; seen labels so far: [0]\n",
      "Processed 7600 samples; seen labels so far: [0]\n",
      "Processed 7700 samples; seen labels so far: [0]\n",
      "Processed 7800 samples; seen labels so far: [0]\n",
      "Processed 7900 samples; seen labels so far: [0]\n",
      "Processed 8000 samples; seen labels so far: [0]\n",
      "Processed 8100 samples; seen labels so far: [0]\n",
      "Processed 8200 samples; seen labels so far: [0]\n",
      "Processed 8300 samples; seen labels so far: [0]\n",
      "Processed 8400 samples; seen labels so far: [0]\n",
      "Processed 8500 samples; seen labels so far: [0]\n",
      "Processed 8600 samples; seen labels so far: [0]\n",
      "Final triggered labels: [0]\n",
      "⚠️ Only triggered 1 out of 2 labels.\n"
     ]
    }
   ],
   "source": [
    "seen_labels = set()\n",
    "max_words = 5000  \n",
    "count = 0\n",
    "\n",
    "for synset in list(wn.all_synsets())[:max_words]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        # 构造简单句子\n",
    "        text = f\"{lemma}\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "            seen_labels.add(pred_label)\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count} samples; seen labels so far: {sorted(seen_labels)}\")\n",
    "        if len(seen_labels) == num_labels:\n",
    "            print(\"✅ All labels triggered!\")\n",
    "            break\n",
    "    if len(seen_labels) == num_labels:\n",
    "        break\n",
    "\n",
    "print(f\"Final triggered labels: {sorted(seen_labels)}\")\n",
    "if len(seen_labels) < num_labels:\n",
    "    print(f\"⚠️ Only triggered {len(seen_labels)} out of {num_labels} labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a9058ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 samples; seen labels so far: [0]\n",
      "Processed 200 samples; seen labels so far: [0]\n",
      "Processed 300 samples; seen labels so far: [0]\n",
      "Processed 400 samples; seen labels so far: [0]\n",
      "Processed 500 samples; seen labels so far: [0]\n",
      "Processed 600 samples; seen labels so far: [0]\n",
      "Processed 700 samples; seen labels so far: [0]\n",
      "Processed 800 samples; seen labels so far: [0]\n",
      "Processed 900 samples; seen labels so far: [0]\n",
      "Processed 1000 samples; seen labels so far: [0]\n",
      "Processed 1100 samples; seen labels so far: [0]\n",
      "Processed 1200 samples; seen labels so far: [0]\n",
      "Processed 1300 samples; seen labels so far: [0]\n",
      "Processed 1400 samples; seen labels so far: [0]\n",
      "Processed 1500 samples; seen labels so far: [0]\n",
      "Processed 1600 samples; seen labels so far: [0]\n",
      "Processed 1700 samples; seen labels so far: [0]\n",
      "Processed 1800 samples; seen labels so far: [0]\n",
      "Processed 1900 samples; seen labels so far: [0]\n",
      "Processed 2000 samples; seen labels so far: [0]\n",
      "Processed 2100 samples; seen labels so far: [0]\n",
      "Processed 2200 samples; seen labels so far: [0]\n",
      "Processed 2300 samples; seen labels so far: [0]\n",
      "Processed 2400 samples; seen labels so far: [0]\n",
      "Processed 2500 samples; seen labels so far: [0]\n",
      "Processed 2600 samples; seen labels so far: [0]\n",
      "Processed 2700 samples; seen labels so far: [0]\n",
      "Processed 2800 samples; seen labels so far: [0]\n",
      "Processed 2900 samples; seen labels so far: [0]\n",
      "Processed 3000 samples; seen labels so far: [0]\n",
      "Processed 3100 samples; seen labels so far: [0]\n",
      "Processed 3200 samples; seen labels so far: [0]\n",
      "Processed 3300 samples; seen labels so far: [0]\n",
      "Processed 3400 samples; seen labels so far: [0]\n",
      "Processed 3500 samples; seen labels so far: [0]\n",
      "Processed 3600 samples; seen labels so far: [0]\n",
      "Processed 3700 samples; seen labels so far: [0]\n",
      "Processed 3800 samples; seen labels so far: [0]\n",
      "Processed 3900 samples; seen labels so far: [0]\n",
      "Processed 4000 samples; seen labels so far: [0]\n",
      "Processed 4100 samples; seen labels so far: [0]\n",
      "Processed 4200 samples; seen labels so far: [0]\n",
      "Processed 4300 samples; seen labels so far: [0]\n",
      "Processed 4400 samples; seen labels so far: [0]\n",
      "Processed 4500 samples; seen labels so far: [0]\n",
      "Processed 4600 samples; seen labels so far: [0]\n",
      "Processed 4700 samples; seen labels so far: [0]\n",
      "Processed 4800 samples; seen labels so far: [0]\n",
      "Processed 4900 samples; seen labels so far: [0]\n",
      "Processed 5000 samples; seen labels so far: [0]\n",
      "Processed 5100 samples; seen labels so far: [0]\n",
      "Processed 5200 samples; seen labels so far: [0]\n",
      "Processed 5300 samples; seen labels so far: [0]\n",
      "Processed 5400 samples; seen labels so far: [0]\n",
      "Processed 5500 samples; seen labels so far: [0]\n",
      "Processed 5600 samples; seen labels so far: [0]\n",
      "Processed 5700 samples; seen labels so far: [0]\n",
      "Processed 5800 samples; seen labels so far: [0]\n",
      "Processed 5900 samples; seen labels so far: [0]\n",
      "Processed 6000 samples; seen labels so far: [0]\n",
      "Processed 6100 samples; seen labels so far: [0]\n",
      "Processed 6200 samples; seen labels so far: [0]\n",
      "Processed 6300 samples; seen labels so far: [0]\n",
      "Processed 6400 samples; seen labels so far: [0]\n",
      "Processed 6500 samples; seen labels so far: [0]\n",
      "Processed 6600 samples; seen labels so far: [0]\n",
      "Processed 6700 samples; seen labels so far: [0]\n",
      "Processed 6800 samples; seen labels so far: [0]\n",
      "Processed 6900 samples; seen labels so far: [0]\n",
      "Processed 7000 samples; seen labels so far: [0]\n",
      "Processed 7100 samples; seen labels so far: [0]\n",
      "Processed 7200 samples; seen labels so far: [0]\n",
      "Processed 7300 samples; seen labels so far: [0]\n",
      "Processed 7400 samples; seen labels so far: [0]\n",
      "Processed 7500 samples; seen labels so far: [0]\n",
      "Processed 7600 samples; seen labels so far: [0]\n",
      "Processed 7700 samples; seen labels so far: [0]\n",
      "Processed 7800 samples; seen labels so far: [0]\n",
      "Processed 7900 samples; seen labels so far: [0]\n",
      "Processed 8000 samples; seen labels so far: [0]\n",
      "Processed 8100 samples; seen labels so far: [0]\n",
      "Processed 8200 samples; seen labels so far: [0]\n",
      "Processed 8300 samples; seen labels so far: [0]\n",
      "Processed 8400 samples; seen labels so far: [0]\n",
      "Processed 8500 samples; seen labels so far: [0]\n",
      "Processed 8600 samples; seen labels so far: [0]\n",
      "Final triggered labels: [0]\n",
      "⚠️ Only triggered 1 out of 2 labels.\n"
     ]
    }
   ],
   "source": [
    "## 加空格\n",
    "seen_labels = set()\n",
    "max_words = 5000  \n",
    "count = 0\n",
    "\n",
    "for synset in list(wn.all_synsets())[:max_words]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        # 构造简单句子\n",
    "        text = f\"  {lemma}\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "            seen_labels.add(pred_label)\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count} samples; seen labels so far: {sorted(seen_labels)}\")\n",
    "        if len(seen_labels) == num_labels:\n",
    "            print(\"✅ All labels triggered!\")\n",
    "            break\n",
    "    if len(seen_labels) == num_labels:\n",
    "        break\n",
    "\n",
    "print(f\"Final triggered labels: {sorted(seen_labels)}\")\n",
    "if len(seen_labels) < num_labels:\n",
    "    print(f\"⚠️ Only triggered {len(seen_labels)} out of {num_labels} labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b509663e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 samples; seen labels so far: [0]\n",
      "Processed 200 samples; seen labels so far: [0]\n",
      "Processed 300 samples; seen labels so far: [0]\n",
      "Processed 400 samples; seen labels so far: [0]\n",
      "✅ All labels triggered!\n",
      "Final triggered labels: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "## 重复\n",
    "seen_labels = set()\n",
    "max_words = 5000  \n",
    "count = 0\n",
    "\n",
    "for synset in list(wn.all_synsets())[:max_words]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        # 构造简单句子\n",
    "        text = f\"{lemma} {lemma} {lemma}\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "            seen_labels.add(pred_label)\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count} samples; seen labels so far: {sorted(seen_labels)}\")\n",
    "        if len(seen_labels) == num_labels:\n",
    "            print(\"✅ All labels triggered!\")\n",
    "            break\n",
    "    if len(seen_labels) == num_labels:\n",
    "        break\n",
    "\n",
    "print(f\"Final triggered labels: {sorted(seen_labels)}\")\n",
    "if len(seen_labels) < num_labels:\n",
    "    print(f\"⚠️ Only triggered {len(seen_labels)} out of {num_labels} labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5c8083",
   "metadata": {},
   "source": [
    "# Sarcasm-classifier\n",
    "    https://huggingface.co/hallisky/sarcasm-classifier-gpt4-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bf9bd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# 1. 加载模型与 tokenizer\n",
    "model_name = \"hallisky/sarcasm-classifier-gpt4-data\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()\n",
    "# 2. 获取模型标签数\n",
    "num_labels = model.config.num_labels\n",
    "print(f\"Number of labels: {num_labels}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "993143e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 samples; seen labels so far: [0]\n",
      "Processed 200 samples; seen labels so far: [0]\n",
      "Processed 300 samples; seen labels so far: [0]\n",
      "Processed 400 samples; seen labels so far: [0]\n",
      "Processed 500 samples; seen labels so far: [0]\n",
      "✅ All labels triggered!\n",
      "Final triggered labels: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "seen_labels = set()\n",
    "max_words = 5000  \n",
    "count = 0\n",
    "\n",
    "for synset in list(wn.all_synsets())[:max_words]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        # 构造简单句子\n",
    "        text = f\"{lemma}\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "            seen_labels.add(pred_label)\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count} samples; seen labels so far: {sorted(seen_labels)}\")\n",
    "        if len(seen_labels) == num_labels:\n",
    "            print(\"✅ All labels triggered!\")\n",
    "            break\n",
    "    if len(seen_labels) == num_labels:\n",
    "        break\n",
    "\n",
    "print(f\"Final triggered labels: {sorted(seen_labels)}\")\n",
    "if len(seen_labels) < num_labels:\n",
    "    print(f\"⚠️ Only triggered {len(seen_labels)} out of {num_labels} labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc2303f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/j-hartmann/emotion-english-distilroberta-base/xet-read-token/0e1cd914e3d46199ed785853e12b57304e04178b (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7ff61a110a10>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: d3785ba2-d56b-4262-9cdb-5847503bc787)')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/bullying/lib/python3.11/site-packages/urllib3/connection.py:199\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 199\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/bullying/lib/python3.11/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bullying/lib/python3.11/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 101] Network is unreachable",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/bullying/lib/python3.11/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bullying/lib/python3.11/site-packages/urllib3/connectionpool.py:490\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    489\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[0;32m--> 490\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[1;32m    492\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bullying/lib/python3.11/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/bullying/lib/python3.11/site-packages/urllib3/connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1095\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bullying/lib/python3.11/site-packages/urllib3/connection.py:693\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    692\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[0;32m--> 693\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[0;32m~/anaconda3/envs/bullying/lib/python3.11/site-packages/urllib3/connection.py:214\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    216\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Audit hooks are only available in Python 3.8+\u001b[39;00m\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x7ff61a110a10>: Failed to establish a new connection: [Errno 101] Network is unreachable",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/bullying/lib/python3.11/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/bullying/lib/python3.11/site-packages/urllib3/connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    841\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 843\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/anaconda3/envs/bullying/lib/python3.11/site-packages/urllib3/util/retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/j-hartmann/emotion-english-distilroberta-base/xet-read-token/0e1cd914e3d46199ed785853e12b57304e04178b (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7ff61a110a10>: Failed to establish a new connection: [Errno 101] Network is unreachable'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mj-hartmann/emotion-english-distilroberta-base\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 2. 获取模型标签数\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bullying/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/bullying/lib/python3.11/site-packages/transformers/modeling_utils.py:3854\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3851\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3852\u001b[0m         \u001b[38;5;66;03m# This repo has no safetensors file of any kind, we switch to PyTorch.\u001b[39;00m\n\u001b[1;32m   3853\u001b[0m         filename \u001b[38;5;241m=\u001b[39m _add_variant(WEIGHTS_NAME, variant)\n\u001b[0;32m-> 3854\u001b[0m         resolved_archive_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3855\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcached_file_kwargs\u001b[49m\n\u001b[1;32m   3856\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(WEIGHTS_NAME, variant):\n\u001b[1;32m   3858\u001b[0m     \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n\u001b[1;32m   3859\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[1;32m   3860\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3861\u001b[0m         _add_variant(WEIGHTS_INDEX_NAME, variant),\n\u001b[1;32m   3862\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcached_file_kwargs,\n\u001b[1;32m   3863\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/bullying/lib/python3.11/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    418\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m~/anaconda3/envs/bullying/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/bullying/lib/python3.11/site-packages/huggingface_hub/file_download.py:1008\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m    989\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m    990\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1006\u001b[0m     )\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/bullying/lib/python3.11/site-packages/huggingface_hub/file_download.py:1161\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001b[39;00m\n\u001b[1;32m   1160\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1161\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[1;32m   1174\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/bullying/lib/python3.11/site-packages/huggingface_hub/file_download.py:1710\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[0m\n\u001b[1;32m   1708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_xet_available():\n\u001b[1;32m   1709\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXet Storage is enabled for this repo. Downloading file from Xet Storage..\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1710\u001b[0m     \u001b[43mxet_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mincomplete_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisplayed_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1716\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1717\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1718\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/bullying/lib/python3.11/site-packages/huggingface_hub/file_download.py:592\u001b[0m, in \u001b[0;36mxet_get\u001b[0;34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, _tqdm_bar)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    588\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use optimized download using Xet storage, you need to install the hf_xet package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    589\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTry `pip install \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuggingface_hub[hf_xet]\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m` or `pip install hf_xet`.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    590\u001b[0m     )\n\u001b[0;32m--> 592\u001b[0m connection_info \u001b[38;5;241m=\u001b[39m \u001b[43mrefresh_xet_connection_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtoken_refresher\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    595\u001b[0m     connection_info \u001b[38;5;241m=\u001b[39m refresh_xet_connection_info(file_data\u001b[38;5;241m=\u001b[39mxet_file_data, headers\u001b[38;5;241m=\u001b[39mheaders)\n",
      "File \u001b[0;32m~/anaconda3/envs/bullying/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/bullying/lib/python3.11/site-packages/huggingface_hub/utils/_xet.py:112\u001b[0m, in \u001b[0;36mrefresh_xet_connection_info\u001b[0;34m(file_data, headers)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_data\u001b[38;5;241m.\u001b[39mrefresh_route \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe provided xet metadata does not contain a refresh endpoint.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_fetch_xet_connection_info_with_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefresh_route\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/bullying/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/bullying/lib/python3.11/site-packages/huggingface_hub/utils/_xet.py:182\u001b[0m, in \u001b[0;36m_fetch_xet_connection_info_with_url\u001b[0;34m(url, headers, params)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;129m@validate_hf_hub_args\u001b[39m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fetch_xet_connection_info_with_url\u001b[39m(\n\u001b[1;32m    159\u001b[0m     url: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    160\u001b[0m     headers: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    161\u001b[0m     params: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    162\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m XetConnectionInfo:\n\u001b[1;32m    163\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    Requests the xet connection info from the supplied URL. This includes the\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m    access token, expiration time, and endpoint to use for the xet storage service.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m            If the Hub API response is improperly formatted.\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m     hf_raise_for_status(resp)\n\u001b[1;32m    185\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m parse_xet_connection_info_from_headers(resp\u001b[38;5;241m.\u001b[39mheaders)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bullying/lib/python3.11/site-packages/requests/sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \n\u001b[1;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/bullying/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/bullying/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/envs/bullying/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:96\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_curlify(request)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m~/anaconda3/envs/bullying/lib/python3.11/site-packages/requests/adapters.py:700\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    697\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectionError\u001b[0m: (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/j-hartmann/emotion-english-distilroberta-base/xet-read-token/0e1cd914e3d46199ed785853e12b57304e04178b (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7ff61a110a10>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: d3785ba2-d56b-4262-9cdb-5847503bc787)')"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# 1. 加载模型与 tokenizer\n",
    "model_name = \"j-hartmann/emotion-english-distilroberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()\n",
    "# 2. 获取模型标签数\n",
    "num_labels = model.config.num_labels\n",
    "print(f\"Number of labels: {num_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e831627",
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_labels = set()\n",
    "max_words = 5000  \n",
    "count = 0\n",
    "\n",
    "for synset in list(wn.all_synsets())[:max_words]:\n",
    "    for lemma in synset.lemma_names():\n",
    "        # 构造简单句子\n",
    "        text = f\"{lemma}\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "            seen_labels.add(pred_label)\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count} samples; seen labels so far: {sorted(seen_labels)}\")\n",
    "        if len(seen_labels) == num_labels:\n",
    "            print(\"✅ All labels triggered!\")\n",
    "            break\n",
    "    if len(seen_labels) == num_labels:\n",
    "        break\n",
    "\n",
    "print(f\"Final triggered labels: {sorted(seen_labels)}\")\n",
    "if len(seen_labels) < num_labels:\n",
    "    print(f\"⚠️ Only triggered {len(seen_labels)} out of {num_labels} labels.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bullying",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
