"""Learnable prompt vector for domain discovery."""

import torch
import torch.nn as nn
import numpy as np
from typing import Optional, List, Dict, Tuple, Set, Union
from nltk.corpus import wordnet as wn
import nltk
import random
from collections import defaultdict
from pathlib import Path
from blackbox import BlackBox
from sklearn.cluster import KMeans
from sklearn.metrics import pairwise_distances_argmin_min
from sentence_transformers import SentenceTransformer
from torch.utils.data import DataLoader, Dataset

class SimpleEmbedder:
    """ç®€å•çš„æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼Œç”Ÿæˆ128ç»´åµŒå…¥å‘é‡
    
    è¿™ä¸ªç±»ä½¿ç”¨é¢„è®­ç»ƒçš„Sentence-BERTæ¨¡å‹ç”ŸæˆåµŒå…¥å‘é‡ï¼Œ
    å¹¶é€šè¿‡çº¿æ€§å±‚å°†å…¶æŠ•å½±åˆ°128ç»´ï¼Œä»¥ä¸diffusionæ¨¡å‹å…¼å®¹ã€‚
    
    Args:
        target_dim: ç›®æ ‡åµŒå…¥ç»´åº¦ï¼Œé»˜è®¤ä¸º128
        use_cache: æ˜¯å¦ç¼“å­˜åµŒå…¥ç»“æœ
    """
    
    def __init__(self, target_dim: int = 128, use_cache: bool = True):
        self.target_dim = target_dim
        self.use_cache = use_cache
        self.cache = {}
        
        # å»¶è¿ŸåŠ è½½æ¨¡å‹ï¼Œä»…åœ¨éœ€è¦æ—¶åŠ è½½
        self._model = None
        self._projection = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    def _load_model(self):
        """åŠ è½½é¢„è®­ç»ƒçš„Sentence-BERTæ¨¡å‹"""
        # ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ä»¥æé«˜æ•ˆç‡
        self._model = SentenceTransformer('paraphrase-MiniLM-L6-v2')
        # åˆ›å»ºæŠ•å½±å±‚
        input_dim = self._model.get_sentence_embedding_dimension()
        self._projection = nn.Linear(input_dim, self.target_dim).to(self.device)
        # éšæœºåˆå§‹åŒ–æŠ•å½±å±‚
        nn.init.xavier_uniform_(self._projection.weight)
        nn.init.zeros_(self._projection.bias)
        print(f"åŠ è½½äº†Sentence-BERTæ¨¡å‹å¹¶åˆ›å»ºäº†ä»{input_dim}åˆ°{self.target_dim}çš„æŠ•å½±å±‚")

    def encode(self, text: Union[str, List[str]]) -> torch.Tensor:
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºæŒ‡å®šç»´åº¦çš„åµŒå…¥å‘é‡
        
        Args:
            text: è¾“å…¥æ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            å½¢çŠ¶ä¸º(batch_size, target_dim)çš„åµŒå…¥å‘é‡
        """
        # å¦‚æœæ˜¯å•ä¸ªå­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
        if isinstance(text, str):
            texts = [text]
            single_input = True
        else:
            texts = text
            single_input = False
        
        # æ£€æŸ¥ç¼“å­˜
        if self.use_cache:
            # å°è¯•ä»ç¼“å­˜ä¸­è·å–åµŒå…¥
            cached_embeddings = []
            texts_to_encode = []
            indices = []
            
            for i, t in enumerate(texts):
                if t in self.cache:
                    cached_embeddings.append(self.cache[t])
                else:
                    texts_to_encode.append(t)
                    indices.append(i)
            
            # å¦‚æœæ‰€æœ‰æ–‡æœ¬éƒ½åœ¨ç¼“å­˜ä¸­
            if len(texts_to_encode) == 0:
                embeddings = torch.stack(cached_embeddings)
                if single_input:
                    return embeddings[0].unsqueeze(0)  # æ·»åŠ batchç»´åº¦
                return embeddings
        else:
            texts_to_encode = texts
            indices = list(range(len(texts)))
        
        # å»¶è¿ŸåŠ è½½æ¨¡å‹
        if self._model is None:
            self._load_model()
        
        # ç¼–ç æ–‡æœ¬
        with torch.no_grad():
            # ä½¿ç”¨Sentence-BERTè·å–åµŒå…¥
            embeddings = self._model.encode(texts_to_encode, convert_to_tensor=True)
            # ç¡®ä¿embeddingsåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            embeddings = embeddings.to(self.device)
            # æŠ•å½±åˆ°ç›®æ ‡ç»´åº¦
            embeddings = self._projection(embeddings)
        
        # æ›´æ–°ç¼“å­˜
        if self.use_cache:
            for t, emb in zip(texts_to_encode, embeddings):
                self.cache[t] = emb
            
            # å°†æ–°ç¼–ç çš„åµŒå…¥ä¸ç¼“å­˜çš„åµŒå…¥åˆå¹¶
            all_embeddings = [None] * len(texts)
            for i, emb in enumerate(cached_embeddings):
                # ç¡®ä¿ç¼“å­˜çš„åµŒå…¥ä¹Ÿåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                all_embeddings[i] = emb.to(self.device)
            for i, idx in enumerate(indices):
                all_embeddings[idx] = embeddings[i]
            
            embeddings = torch.stack(all_embeddings)
        
        # å¦‚æœæ˜¯å•ä¸ªè¾“å…¥ï¼Œè¿”å›å•ä¸ªåµŒå…¥
        if single_input:
            return embeddings[0].unsqueeze(0)  # æ·»åŠ batchç»´åº¦
        
        return embeddings


class WordNetConditioner(nn.Module):
    """Learnable prompt vector for steering text generation.
    
    This module wraps a single learnable embedding vector that will be
    optimized to guide the diffusion model toward generating text with
    desired properties.
    
    Args:
        hidden_dim: Dimension of the embedding vector (matches model)
        init_scale: Scale for random initialization
        init_method: åˆå§‹åŒ–æ–¹æ³•ï¼Œå¯é€‰ 'random'(éšæœºåˆå§‹åŒ–) æˆ– 'wordnet'(åŸºäºWordNetåˆå§‹åŒ–)
        embedding_model: ç”¨äºå°†è¯æ±‡è½¬æ¢ä¸ºåµŒå…¥çš„æ¨¡å‹ï¼ˆå¦‚æœinit_method='wordnet'ï¼‰
        max_words: WordNetéå†çš„æœ€å¤§è¯æ±‡æ•°
        min_words_per_category: æ¯ä¸ªç±»åˆ«æœ€å°‘ä¿ç•™çš„è¯æ±‡æ•°é‡
    """
    
    def __init__(
        self,
        hidden_dim: int = 128,  # ä¿®æ”¹ä¸ºä¸ diffusion æ¨¡å‹åŒ¹é…çš„ç»´åº¦
        init_scale: float = 0.02,
        init_method: str = 'wordnet',
        embedding_model = None,
        max_words: int = 5000,
        min_words_per_category: int = 20,
        black_model = None,
    ):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.init_method = init_method
        self.black_model = black_model
        
        if init_method == 'random':
            # éšæœºåˆå§‹åŒ–
            self.embedding = nn.Parameter(
                torch.randn(1, hidden_dim) * init_scale
            )
        elif init_method == 'wordnet':
            # åŸºäºWordNetåˆå§‹åŒ–
            self.embedding_model = SimpleEmbedder()
            self.embeddings_dict, self.embeddings_words_dict = self.initialize_from_wordnet(black_model)
    
    def clone_detached(self) -> torch.Tensor:
        """Return a detached copy of the prompt vector.
        
        Returns:
            Tensor of shape (1, hidden_dim), detached from computation graph
        """
        return self.embedding.detach().clone()
    
    def initialize_from_wordnet(self, black_model=None) -> None:
        """
        ä»WordNetè¯æ±‡åˆå§‹åŒ–prompt vectorï¼Œä¿ç•™æ¯ä¸ªlabelä¸‹æ¯ä¸ªèšç±»ä¸­å¿ƒæœ€é è¿‘çš„è¯ã€‚
        
        Args:
            black_model: å¯é€‰çš„é»‘ç›’æ¨¡å‹ï¼Œç”¨äºè¾…åŠ©é€‰æ‹©è¯æ±‡ã€‚
        Returns:
            embeddings_dict: æ¯ä¸ªlabelä¸‹5ä¸ªä¸­å¿ƒè¯çš„embeddingï¼Œshape: [5, hidden_dim]
            embeddings_words_dict: æ¯ä¸ªlabelä¸‹çš„5ä¸ªä¸­å¿ƒè¯ï¼ˆå­—ç¬¦ä¸²ï¼‰
        """

        word_dict = traverse_wordnet(
            black_model=black_model,
            max_words=5000,
            min_words_per_label=20
        )

        embeddings_dict = {}
        embeddings_words_dict = {}

        for label, words in word_dict.items():
            print(f"æ‰¾åˆ° {len(words)} ä¸ªæ ‡ç­¾ä¸º {label} çš„è¯æ±‡")

            embeddings = []
            word_list = []

            for word in words:
                with torch.no_grad():
                    embedding = self.embedding_model.encode(word)
                    if isinstance(embedding, torch.Tensor) and embedding.numel() == self.hidden_dim:
                        embeddings.append(embedding.reshape(1, -1))
                        word_list.append(word)

            if len(embeddings) < 2:
                print(f"âš ï¸ ç±»åˆ« {label} ä¸­æœ‰æ•ˆåµŒå…¥è¯æ±‡æ•°é‡ä¸è¶³ï¼Œè·³è¿‡èšç±»")
                continue

            embeddings = torch.cat(embeddings, dim=0).cpu().numpy()

            n_clusters = min(5, len(embeddings))
            kmeans = KMeans(n_clusters=n_clusters, random_state=42)
            kmeans.fit(embeddings)

            print(f"æ ‡ç­¾ {label} çš„èšç±»ç»“æœï¼ˆå…± {n_clusters} ç±»ï¼‰ï¼š")

            center_embeddings = []
            center_words = []

            for cluster_id in range(n_clusters):
                indices = [i for i, pred in enumerate(kmeans.labels_) if pred == cluster_id]
                cluster_embeddings = embeddings[indices]
                cluster_words = [word_list[i] for i in indices]

                # æ‰¾å‡ºæœ€é è¿‘èšç±»ä¸­å¿ƒçš„è¯
                center = kmeans.cluster_centers_[cluster_id].reshape(1, -1)
                closest_idx, _ = pairwise_distances_argmin_min(center, cluster_embeddings)
                center_word = cluster_words[closest_idx[0]]
                center_embed = cluster_embeddings[closest_idx[0]]

                center_words.append(center_word)
                center_embeddings.append(center_embed)

                print(f"  ğŸ”¸ Cluster {cluster_id}: {center_word}")

            # ä¿å­˜æœ€ç»ˆç»“æœï¼šæ¯ä¸ªlabelä¸‹çš„ä¸­å¿ƒè¯å’Œå…¶å¯¹åº”çš„embedding
            embeddings_dict[label] = np.stack(center_embeddings, axis=0)
            embeddings_words_dict[label] = center_words

        return embeddings_dict, embeddings_words_dict

def traverse_wordnet(black_model=None, max_words=5000, min_words_per_label=20):
    """éå†WordNetè¯æ±‡ï¼Œæ‰¾åˆ°é»‘ç›’æ¨¡å‹é¢„æµ‹ä¸ºç›®æ ‡æ ‡ç­¾çš„è¯æ±‡
    
    Args:
        black_model: é»‘ç›’æ¨¡å‹ï¼Œç”¨äºé¢„æµ‹è¯æ±‡çš„æ ‡ç­¾
        max_words: æœ€å¤§å¤„ç†è¯æ±‡æ•°é‡
        min_words_per_label: æ¯ä¸ªæ ‡ç­¾æœ€å°‘ä¿ç•™çš„è¯æ±‡æ•°é‡
        
    Returns:
        å¦‚æœtarget_labelä¸ºNoneï¼Œè¿”å›{label: [words]}çš„å­—å…¸
        å¦‚æœtarget_labelä¸ä¸ºNoneï¼Œè¿”å›ç¬¦åˆç›®æ ‡æ ‡ç­¾çš„è¯æ±‡åˆ—è¡¨
    """
    
    # è·å–æ‰€æœ‰åŒä¹‰è¯é›†
    all_synsets = list(wn.all_synsets())
    print(f"WordNetä¸­å…±æœ‰ {len(all_synsets)} ä¸ªåŒä¹‰è¯é›†")
    
    label_to_words = defaultdict(set)
    count = 0

    for synset in all_synsets[:max_words]:
        for lemma in synset.lemma_names():
            word = lemma.replace('_', ' ')  # å°†ä¸‹åˆ’çº¿æ›¿æ¢ä¸ºç©ºæ ¼ï¼Œä½¿å…¶æ›´è‡ªç„¶
            try:
                pred_label = black_model.predict([word])[0]
                label_to_words[pred_label].add(word)
            except Exception as e:
                print(f"âš ï¸ Prediction failed for word: {word} | Error: {e}")
                continue

            count += 1
            if count % 200 == 0:
                current_status = {k: len(v) for k, v in label_to_words.items()}
                print(f"Processed {count} samples; label stats: {current_status}")

        # å¦‚æœæ‰€æœ‰æ ‡ç­¾éƒ½æ»¡è¶³æœ€å°è¯æ•°è¦æ±‚ï¼Œåˆ™æå‰ç»ˆæ­¢
        if all(len(label_to_words[label]) >= min_words_per_label for label in range(black_model.num_labels)):
            break

    # æœ€ç»ˆè¿‡æ»¤ä¸æ»¡è¶³è¦æ±‚çš„æ ‡ç­¾
    filtered_result = {
        label: words for label, words in label_to_words.items()
        if len(words) >= min_words_per_label
    }

    print(f"æœ€ç»ˆä¿ç•™æ ‡ç­¾æ•°: {len(filtered_result)}")
    return filtered_result



class TextDataset(Dataset):
    def __init__(self, text_datasets, data_args, model_emb=None):
        super().__init__()
        input_ids = text_datasets['input_ids']
        input_mask = text_datasets['input_mask']
        
        # æ„é€  list of dictsï¼Œæ¯æ¡æ ·æœ¬æ˜¯ä¸€ä¸ªå­—å…¸
        self.text_datasets = [
            {'input_ids': input_ids[i], 'input_mask': input_mask[i]}
            for i in range(len(input_ids))
        ]
        self.length = len(self.text_datasets)
        self.data_args = data_args
        self.model_emb = model_emb

    def __len__(self):
        return self.length

    def __getitem__(self, idx):
        with torch.no_grad():
            input_ids = self.text_datasets[idx]['input_ids']
            # ç›´æ¥ä½¿ç”¨CPUå¤„ç†ï¼Œé¿å…å¤šè¿›ç¨‹ä¸­çš„CUDAé—®é¢˜
            input_tensor = torch.tensor(input_ids)
            
            # ä½¿ç”¨CPUè®¡ç®—
            hidden_state = self.model_emb(input_tensor)
            arr = np.array(hidden_state.detach().numpy(), dtype=np.float32)

            out_kwargs = {
                'input_ids': np.array(input_ids),
                'input_mask': np.array(self.text_datasets[idx]['input_mask']),
            }

            return arr, out_kwargs




if __name__ == "__main__":

    # æµ‹è¯•éšæœºåˆå§‹åŒ–
    print("\næµ‹è¯•éšæœºåˆå§‹åŒ–...")
    black_model = BlackBox("j-hartmann/emotion-english-distilroberta-base")
    prompt = WordNetConditioner(hidden_dim=128, init_method='wordnet', black_model=black_model)  # ä¿®æ”¹ä¸ºä¸ diffusion æ¨¡å‹åŒ¹é…çš„ç»´åº¦
    embeddings_dict = prompt.embeddings_dict
    words_dict = prompt.embeddings_words_dict
    # print(embeddings_dict)
    print(words_dict)
    
