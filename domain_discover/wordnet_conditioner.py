"""Learnable prompt vector for domain discovery."""

import torch
import torch.nn as nn
import numpy as np
from typing import Optional, List, Dict, Tuple, Set, Union
from nltk.corpus import wordnet as wn
import nltk
import random
from collections import defaultdict
from pathlib import Path
from blackbox import BlackBox
from sklearn.cluster import KMeans
from sklearn.metrics import pairwise_distances_argmin_min
from sentence_transformers import SentenceTransformer
from torch.utils.data import DataLoader, Dataset
from transformers import T5Tokenizer, T5EncoderModel, GPT2Tokenizer, GPT2Model, AutoTokenizer, AutoModelForCausalLM
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.metrics import silhouette_score
import os
from collections import Counter

class SimpleEmbedder:
    """ç®€å•çš„æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼Œç”Ÿæˆ128ç»´åµŒå…¥å‘é‡
    
    è¿™ä¸ªç±»ä½¿ç”¨é¢„è®­ç»ƒçš„Sentence-BERTæ¨¡å‹ç”ŸæˆåµŒå…¥å‘é‡ï¼Œ
    å¹¶é€šè¿‡çº¿æ€§å±‚å°†å…¶æŠ•å½±åˆ°128ç»´ï¼Œä»¥ä¸diffusionæ¨¡å‹å…¼å®¹ã€‚
    
    Args:
        target_dim: ç›®æ ‡åµŒå…¥ç»´åº¦ï¼Œé»˜è®¤ä¸º128
        use_cache: æ˜¯å¦ç¼“å­˜åµŒå…¥ç»“æœ
    """
    
    def __init__(self, target_dim: int = 128, use_cache: bool = True):
        self.target_dim = target_dim
        self.use_cache = use_cache
        self.cache = {}
        
        # å»¶è¿ŸåŠ è½½æ¨¡å‹ï¼Œä»…åœ¨éœ€è¦æ—¶åŠ è½½
        self._model = None
        self._projection = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    def _load_model(self):
        """åŠ è½½é¢„è®­ç»ƒçš„Sentence-BERTæ¨¡å‹"""
        # ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ä»¥æé«˜æ•ˆç‡
        self._model = SentenceTransformer('paraphrase-MiniLM-L6-v2')
        # åˆ›å»ºæŠ•å½±å±‚
        input_dim = self._model.get_sentence_embedding_dimension()
        self._projection = nn.Linear(input_dim, self.target_dim).to(self.device)
        # éšæœºåˆå§‹åŒ–æŠ•å½±å±‚
        nn.init.xavier_uniform_(self._projection.weight)
        nn.init.zeros_(self._projection.bias)
        print(f"åŠ è½½äº†Sentence-BERTæ¨¡å‹å¹¶åˆ›å»ºäº†ä»{input_dim}åˆ°{self.target_dim}çš„æŠ•å½±å±‚")

    def encode(self, text: Union[str, List[str]]) -> torch.Tensor:
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºæŒ‡å®šç»´åº¦çš„åµŒå…¥å‘é‡
        
        Args:
            text: è¾“å…¥æ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            å½¢çŠ¶ä¸º(batch_size, target_dim)çš„åµŒå…¥å‘é‡
        """
        # å¦‚æœæ˜¯å•ä¸ªå­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
        if isinstance(text, str):
            texts = [text]
            single_input = True
        else:
            texts = text
            single_input = False
        
        # æ£€æŸ¥ç¼“å­˜
        if self.use_cache:
            # å°è¯•ä»ç¼“å­˜ä¸­è·å–åµŒå…¥
            cached_embeddings = []
            texts_to_encode = []
            indices = []
            
            for i, t in enumerate(texts):
                if t in self.cache:
                    cached_embeddings.append(self.cache[t])
                else:
                    texts_to_encode.append(t)
                    indices.append(i)
            
            # å¦‚æœæ‰€æœ‰æ–‡æœ¬éƒ½åœ¨ç¼“å­˜ä¸­
            if len(texts_to_encode) == 0:
                embeddings = torch.stack(cached_embeddings)
                if single_input:
                    return embeddings[0].unsqueeze(0)  # æ·»åŠ batchç»´åº¦
                return embeddings
        else:
            texts_to_encode = texts
            indices = list(range(len(texts)))
        
        # å»¶è¿ŸåŠ è½½æ¨¡å‹
        if self._model is None:
            self._load_model()
        
        # ç¼–ç æ–‡æœ¬
        with torch.no_grad():
            # ä½¿ç”¨Sentence-BERTè·å–åµŒå…¥
            embeddings = self._model.encode(texts_to_encode, convert_to_tensor=True, show_progress_bar=False)
            # ç¡®ä¿embeddingsåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            embeddings = embeddings.to(self.device)
            # æŠ•å½±åˆ°ç›®æ ‡ç»´åº¦
            embeddings = self._projection(embeddings)
        
        # æ›´æ–°ç¼“å­˜
        if self.use_cache:
            for t, emb in zip(texts_to_encode, embeddings):
                self.cache[t] = emb
            
            # å°†æ–°ç¼–ç çš„åµŒå…¥ä¸ç¼“å­˜çš„åµŒå…¥åˆå¹¶
            all_embeddings = [None] * len(texts)
            for i, emb in enumerate(cached_embeddings):
                # ç¡®ä¿ç¼“å­˜çš„åµŒå…¥ä¹Ÿåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                all_embeddings[i] = emb.to(self.device)
            for i, idx in enumerate(indices):
                all_embeddings[idx] = embeddings[i]
            
            embeddings = torch.stack(all_embeddings)
        
        # å¦‚æœæ˜¯å•ä¸ªè¾“å…¥ï¼Œè¿”å›å•ä¸ªåµŒå…¥
        if single_input:
            return embeddings[0].unsqueeze(0)  # æ·»åŠ batchç»´åº¦
        
        return embeddings


class WordNetConditioner(nn.Module):
    """Learnable prompt vector for steering text generation.
    
    This module wraps a single learnable embedding vector that will be
    optimized to guide the diffusion model toward generating text with
    desired properties.
    
    Args:
        hidden_dim: Dimension of the embedding vector (matches model)
        init_scale: Scale for random initialization
        init_method: åˆå§‹åŒ–æ–¹æ³•ï¼Œå¯é€‰ 'random'(éšæœºåˆå§‹åŒ–) æˆ– 'wordnet'(åŸºäºWordNetåˆå§‹åŒ–)
        embedding_model: ç”¨äºå°†è¯æ±‡è½¬æ¢ä¸ºåµŒå…¥çš„æ¨¡å‹ï¼ˆå¦‚æœinit_method='wordnet'ï¼‰
        max_words: WordNetéå†çš„æœ€å¤§è¯æ±‡æ•°
        min_words_per_category: æ¯ä¸ªç±»åˆ«æœ€å°‘ä¿ç•™çš„è¯æ±‡æ•°é‡
    """
    
    def __init__(
        self,
        hidden_dim: int = 128,  # ä¿®æ”¹ä¸ºä¸ diffusion æ¨¡å‹åŒ¹é…çš„ç»´åº¦
        init_scale: float = 0.02,
        init_method: str = 'wordnet',
        embedding_model = None,
        max_words: int = 5000,
        min_words_per_category: int = 20,
        visual: bool = False,
        black_model = None,
        load_from_wordnet = True,
    ):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.init_method = init_method
        self.black_model = black_model
        self.visual = visual
        self.load_from_wordnet = load_from_wordnet

        if init_method == 'random':
            # éšæœºåˆå§‹åŒ–
            self.embedding = nn.Parameter(
                torch.randn(1, hidden_dim) * init_scale
            )
        elif init_method == 'wordnet':
            # åŸºäºWordNetåˆå§‹åŒ–
            self.embedding_model = SimpleEmbedder()
            self.embeddings_dict, self.embeddings_words_dict = self.initialize_from_wordnet(self.black_model)
    
    def clone_detached(self) -> torch.Tensor:
        """Return a detached copy of the prompt vector.
        
        Returns:
            Tensor of shape (1, hidden_dim), detached from computation graph
        """
        return self.embedding.detach().clone()
    
    def get_embeddings(self, word_dict, model_name="T5"):
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        embedding_records = []
        if model_name == "Bert":
            model
            
        if model_name == "qwen":

            model_name = "Qwen/Qwen1.5-7B-Chat"

            # åŠ è½½ tokenizer å’Œæ¨¡å‹
            tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16,
                device_map=device,
                trust_remote_code=True
            )
            model.eval()

            embedding_records = []

            for label, words in word_dict.items():
                for word in words:
                    inputs = tokenizer(word, return_tensors="pt").to(device)
                    input_ids = inputs["input_ids"][0]

                    # æ’é™¤ç‰¹æ®Š tokenï¼ˆå¦‚ padã€eosï¼Œå¦‚æœå­˜åœ¨ï¼‰
                    special_ids = set([
                        tokenizer.pad_token_id,
                        tokenizer.eos_token_id,
                        tokenizer.bos_token_id,
                        tokenizer.unk_token_id
                    ])
                    valid_mask = torch.tensor(
                        [(tok_id.item() not in special_ids) for tok_id in input_ids],
                        device=input_ids.device
                    )
                    valid_indices = valid_mask.nonzero(as_tuple=True)[0]

                    if len(valid_indices) == 0:
                        continue

                    with torch.no_grad():
                        outputs = model(**inputs, output_hidden_states=True, return_dict=True)
                        last_hidden = outputs.hidden_states[-1][0]  # shape: [seq_len, hidden_size]
                        word_embedding = last_hidden[valid_indices].mean(dim=0)

                    embedding_records.append({
                        "embedding": word_embedding.cpu().float().numpy(),
                        "label": label,
                        "word": word
                    })

        if model_name == "T5":
            tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-large")
            encoder = T5EncoderModel.from_pretrained("google/flan-t5-large").to(device)
            encoder.eval()

            for label, words in word_dict.items():
                for word in words:
                    inputs = tokenizer(word, return_tensors="pt").to(device)
                    input_ids = inputs["input_ids"][0]
                    valid_mask = (input_ids != tokenizer.pad_token_id) & (input_ids != tokenizer.eos_token_id)
                    valid_indices = valid_mask.nonzero(as_tuple=True)[0]
                    if len(valid_indices) == 0:
                        continue
                    with torch.no_grad():
                        outputs = encoder(**inputs)
                        last_hidden = outputs.last_hidden_state[0]
                        word_embedding = last_hidden[valid_indices].mean(dim=0)
                    embedding_records.append({
                        "embedding": word_embedding.cpu().numpy(),
                        "label": label,
                        "word": word
                    })

        elif model_name == "GPT2":
            tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
            model = GPT2Model.from_pretrained("gpt2").to(device)
            model.eval()

            for label, words in word_dict.items():
                for word in words:
                    inputs = tokenizer(word, return_tensors="pt", add_special_tokens=False).to(device)
                    input_ids = inputs["input_ids"][0]
                    if input_ids.shape[0] == 0:
                        continue
                    with torch.no_grad():
                        outputs = model(**inputs)
                        hidden_states = outputs.last_hidden_state[0]
                        word_embedding = hidden_states.mean(dim=0)
                    embedding_records.append({
                        "embedding": word_embedding.cpu().numpy(),
                        "label": label,
                        "word": word
                    })

        return embedding_records



    def visual_embedding(self, embedding_records, model_name="T5"):

        # æå–çŸ©é˜µä¸æ ‡ç­¾
        X = torch.tensor([item["embedding"] for item in embedding_records])
        labels = [item["label"] for item in embedding_records]
        label_set = sorted(set(labels))
        colors = plt.cm.get_cmap("tab10", len(label_set))
        score = silhouette_score(X, labels)
        print(f"{model_name} â†’ Silhouette Score: {score:.4f}")  
        # ========= 2D PCA ==========
        pca_2d = PCA(n_components=2)
        X_pca_2d = pca_2d.fit_transform(X)
        plt.figure(figsize=(10, 8))
        for label in label_set:
            idxs = [i for i, l in enumerate(labels) if l == label]
            plt.scatter(X_pca_2d[idxs, 0], X_pca_2d[idxs, 1], label=f"Class {label}", alpha=0.7, s=60)
        plt.title("PCA (2D) of {model_name} Encoder Hidden States")
        plt.xlabel("PC1")
        plt.ylabel("PC2")
        plt.legend()
        plt.grid(True)
        plt.savefig(f"/home/dora/Domain-Inference/domain_discover/output/{model_name}_wordnet_pca_2d_{self.black_model.model_name}.png")
        plt.show()

        # ========= 3D PCA ==========
        pca_3d = PCA(n_components=3)
        X_pca_3d = pca_3d.fit_transform(X)
        fig = plt.figure(figsize=(10, 8))
        ax = fig.add_subplot(111, projection='3d')
        for label in label_set:
            idxs = [i for i, l in enumerate(labels) if l == label]
            ax.scatter(X_pca_3d[idxs, 0], X_pca_3d[idxs, 1], X_pca_3d[idxs, 2], label=f"Class {label}", alpha=0.7, s=30)
        ax.set_title("PCA (3D) of {model_name} Encoder Hidden States")
        ax.set_xlabel("PC1")
        ax.set_ylabel("PC2")
        ax.set_zlabel("PC3")
        ax.legend()
        plt.savefig(f"/home/dora/Domain-Inference/domain_discover/output/{model_name}_wordnet_pca_3d_{self.black_model.model_name}.png")
        plt.show()

        # ========= 2D t-SNE ==========
        tsne_2d = TSNE(n_components=2, perplexity=30, init='random', random_state=42)
        X_tsne_2d = tsne_2d.fit_transform(X)
        plt.figure(figsize=(10, 8))
        for label in label_set:
            idxs = [i for i, l in enumerate(labels) if l == label]
            plt.scatter(X_tsne_2d[idxs, 0], X_tsne_2d[idxs, 1], label=f"Class {label}", alpha=0.7, s=60)
        plt.title("t-SNE (2D) of {model_name} Encoder Hidden States")
        plt.xlabel("Dim 1")
        plt.ylabel("Dim 2")
        plt.legend()
        plt.grid(True)
        plt.savefig(f"/home/dora/Domain-Inference/domain_discover/output/{model_name}_wordnet_tsne_2d_{self.black_model.model_name}.png")
        plt.show()

        # ========= 3D t-SNE ==========
        tsne_3d = TSNE(n_components=3, perplexity=30, init='random', random_state=42)
        X_tsne_3d = tsne_3d.fit_transform(X)
        fig = plt.figure(figsize=(10, 8))
        ax = fig.add_subplot(111, projection='3d')
        for label in label_set:
            idxs = [i for i, l in enumerate(labels) if l == label]
            ax.scatter(X_tsne_3d[idxs, 0], X_tsne_3d[idxs, 1], X_tsne_3d[idxs, 2], label=f"Class {label}", alpha=0.7, s=30)
        ax.set_title("t-SNE (3D) of {model_name} Encoder Hidden States")
        ax.set_xlabel("Dim 1")
        ax.set_ylabel("Dim 2")
        ax.set_zlabel("Dim 3")
        ax.legend()
        plt.savefig(f"/home/dora/Domain-Inference/domain_discover/output/{model_name}_wordnet_tsne_3d_{self.black_model.model_name}.png")
        plt.show()
        return model_name, score

    def cluster_wordnet(self, word_dict):
        embeddings_dict = {}
        embeddings_words_dict = {}

        for label, words in word_dict.items():
            print(f"æ‰¾åˆ° {len(words)} ä¸ªæ ‡ç­¾ä¸º {label} çš„è¯æ±‡")

            embeddings = []
            word_list = []

            for word in words:
                with torch.no_grad():
                    embedding = self.embedding_model.encode(word)
                    if isinstance(embedding, torch.Tensor) and embedding.numel() == self.hidden_dim:
                        embeddings.append(embedding.reshape(1, -1))
                        word_list.append(word)

            if len(embeddings) < 2:
                print(f"âš ï¸ ç±»åˆ« {label} ä¸­æœ‰æ•ˆåµŒå…¥è¯æ±‡æ•°é‡ä¸è¶³ï¼Œè·³è¿‡èšç±»")
                continue

            embeddings = torch.cat(embeddings, dim=0).cpu().numpy()

            n_clusters = min(5, len(embeddings))
            kmeans = KMeans(n_clusters=n_clusters, random_state=42)
            kmeans.fit(embeddings)

            print(f"æ ‡ç­¾ {label} çš„èšç±»ç»“æœï¼ˆå…± {n_clusters} ç±»ï¼‰ï¼š")

            center_embeddings = []
            center_words = []

            for cluster_id in range(n_clusters):
                indices = [i for i, pred in enumerate(kmeans.labels_) if pred == cluster_id]
                cluster_embeddings = embeddings[indices]
                cluster_words = [word_list[i] for i in indices]

                # æ‰¾å‡ºæœ€é è¿‘èšç±»ä¸­å¿ƒçš„è¯
                center = kmeans.cluster_centers_[cluster_id].reshape(1, -1)
                closest_idx, _ = pairwise_distances_argmin_min(center, cluster_embeddings)
                center_word = cluster_words[closest_idx[0]]
                center_embed = cluster_embeddings[closest_idx[0]]

                center_words.append(center_word)
                center_embeddings.append(center_embed)

                print(f"  ğŸ”¸ Cluster {cluster_id}: {center_word}")

        # ä¿å­˜æœ€ç»ˆç»“æœï¼šæ¯ä¸ªlabelä¸‹çš„ä¸­å¿ƒè¯å’Œå…¶å¯¹åº”çš„embedding
        embeddings_dict[label] = np.stack(center_embeddings, axis=0)
        embeddings_words_dict[label] = center_words

        return embeddings_dict, embeddings_words_dict

    
    def initialize_from_wordnet(self, black_model=None) -> None:
        """
        ä»WordNetè¯æ±‡åˆå§‹åŒ–prompt vectorï¼Œä¿ç•™æ¯ä¸ªlabelä¸‹æ¯ä¸ªèšç±»ä¸­å¿ƒæœ€é è¿‘çš„è¯ã€‚
        
        Args:
            black_model: å¯é€‰çš„é»‘ç›’æ¨¡å‹ï¼Œç”¨äºè¾…åŠ©é€‰æ‹©è¯æ±‡ã€‚
        Returns:
            word_dict: æ¯ä¸ªlabelä¸‹çš„è¯æ±‡åˆ—è¡¨
        """

        word_dict = self.traverse_wordnet(
            black_model=black_model,
            max_words=5000,
            min_words_per_label=20
        )
        pca_visual = self.visual

        if pca_visual:
            for model_name in ["T5", "GPT2"]:
                embeddings = self.get_embeddings(word_dict, model_name=model_name)
                model_name, score = self.visual_embedding(embeddings, model_name)
        cluster = False
        if cluster:
            embeddings_dict, embeddings_words_dict = self.cluster_wordnet(word_dict)
            return embeddings_dict, embeddings_words_dict
        else:
            return None, word_dict

    def traverse_wordnet(self, black_model=None, max_words=5000, min_words_per_label=20):
        """éå†WordNetè¯æ±‡ï¼Œæ‰¾åˆ°é»‘ç›’æ¨¡å‹é¢„æµ‹ä¸ºç›®æ ‡æ ‡ç­¾çš„è¯æ±‡
        
        Args:
            black_model: é»‘ç›’æ¨¡å‹ï¼Œç”¨äºé¢„æµ‹è¯æ±‡çš„æ ‡ç­¾
            max_words: æœ€å¤§å¤„ç†è¯æ±‡æ•°é‡
            min_words_per_label: æ¯ä¸ªæ ‡ç­¾æœ€å°‘ä¿ç•™çš„è¯æ±‡æ•°é‡
            
        Returns:
            å¦‚æœtarget_labelä¸ºNoneï¼Œè¿”å›{label: [words]}çš„å­—å…¸
            å¦‚æœtarget_labelä¸ä¸ºNoneï¼Œè¿”å›ç¬¦åˆç›®æ ‡æ ‡ç­¾çš„è¯æ±‡åˆ—è¡¨
        """
        device = "cuda" if torch.cuda.is_available() else "cpu"
        # gpt_model = GPT2LMHeadModel.from_pretrained("gpt2").to(device)
        # tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
        # tokenizer.pad_token = tokenizer.eos_token
        model_name = "Qwen/Qwen1.5-7B-Chat"

        # åŠ è½½ tokenizer å’Œæ¨¡å‹ï¼ˆè®°å¾— trust_remote_code=Trueï¼‰
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,         
            device_map=device,                
            trust_remote_code=True
        )
        words_dict = defaultdict(list)
        # è·å–æ‰€æœ‰åŒä¹‰è¯é›†
        all_synsets = list(wn.all_synsets())
        print(f"WordNetä¸­å…±æœ‰ {len(all_synsets)} ä¸ªåŒä¹‰è¯é›†")
        word_count = 0
        break_outer_loop = False
        if self.load_from_wordnet:
            for synset in all_synsets[:max_words]:
                for lemma in synset.lemma_names():
                    prompt = (
                        f"Write one short English sentence using the word '{lemma}'. Only output the sentence.\n"
                    )
                    good_sentences = []
                    pred_labels = []
                    if word_count % 100 == 0:
                        print(f"å·²æ”¶é›† {word_count} ä¸ªè¯æ±‡")
                    for _ in range(10):
                        inputs = tokenizer(prompt, return_tensors="pt").to(device)
                        output_ids = model.generate(
                            **inputs,
                            max_length=64,
                            temperature=0.8,
                            top_p=0.95,
                            do_sample=True,
                            pad_token_id=tokenizer.eos_token_id
                        )
                        # è§£ç å¹¶å»é™¤ prompt
                        full_output = tokenizer.decode(output_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)
                        if prompt.strip() in full_output:
                            sentence = full_output.replace(prompt.strip(), "").strip()
                        else:
                            sentence = full_output.strip()

                        if not sentence:
                            continue  # skip empty generations

                        # è®°å½•å¥å­å’Œé¢„æµ‹æ ‡ç­¾
                        pred_label = black_model.predict([sentence])[0]
                        good_sentences.append(sentence)
                        pred_labels.append(pred_label)

                    # ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
                    label_counter = Counter(pred_labels)
                    majority_label, count = label_counter.most_common(1)[0]

                    # å¦‚æœ70%ä»¥ä¸Šå±äºåŒä¸€ä¸ªæ ‡ç­¾ï¼Œåˆ™ä¿ç•™
                    if count >= 7:
                        words_dict[majority_label].append(lemma)
                        word_count += 1
                        # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•æ ‡ç­¾ä¸‹çš„è¯æ•°è¶…è¿‡5ä¸ª
                        if all(len(words) > 10 for words in words_dict.values()):
                            break_outer_loop = True
                            break

                        if break_outer_loop:
                            break  # è·³å‡ºå½“å‰çš„å¤–å±‚å¾ªç¯
                if break_outer_loop:
                    break
            
            output_path = "/home/dora/Domain-Inference/domain_discover/data_from_wordnet/words_by_label.txt"
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            with open(output_path, "w", encoding="utf-8") as f:
                for label, lemmas in words_dict.items():
                    f.write(f"Label {int(label)} ({len(lemmas)} words):\n")
                    for lemma in lemmas:
                        f.write(f"  - {lemma}\n")
                    f.write("\n")  # ç©ºè¡Œåˆ†éš”æ ‡ç­¾å—

            return words_dict
        else:
            with open("/home/dora/Domain-Inference/domain_discover/data_from_wordnet/words_by_label.txt", "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if line.startswith("Label"):
                        parts = line.split()
                        current_label = int(parts[1])
                        words_dict[current_label] = []
                    elif line.startswith("- "):  # "  - lemma"
                        lemma = line[2:].strip()
                        words_dict[current_label].append(lemma)
            return words_dict



class TextDataset(Dataset):
    def __init__(self, text_datasets, data_args, model_emb=None):
        super().__init__()
        input_ids = text_datasets['input_ids']
        input_mask = text_datasets['input_mask']
        
        # æ„é€  list of dictsï¼Œæ¯æ¡æ ·æœ¬æ˜¯ä¸€ä¸ªå­—å…¸
        self.text_datasets = [
            {'input_ids': input_ids[i], 'input_mask': input_mask[i]}
            for i in range(len(input_ids))
        ]
        self.length = len(self.text_datasets)
        self.data_args = data_args
        self.model_emb = model_emb

    def __len__(self):
        return self.length

    def __getitem__(self, idx):
        with torch.no_grad():
            input_ids = self.text_datasets[idx]['input_ids']
            input_mask = self.text_datasets[idx]['input_mask']
            
            # ç›´æ¥ä½¿ç”¨CPUå¤„ç†ï¼Œé¿å…å¤šè¿›ç¨‹ä¸­çš„CUDAé—®é¢˜
            input_tensor = torch.tensor(input_ids)
            
            # ä½¿ç”¨CPUè®¡ç®—
            hidden_state = self.model_emb(input_tensor)
            arr = torch.tensor(hidden_state.detach().numpy(), dtype=torch.float32)

            out_kwargs = {
                'input_ids': torch.tensor(input_ids, dtype=torch.long),
                'input_mask': torch.tensor(input_mask, dtype=torch.long),
            }

            return arr, out_kwargs




if __name__ == "__main__":

    # æµ‹è¯•éšæœºåˆå§‹åŒ–
    print("\næµ‹è¯•éšæœºåˆå§‹åŒ–...")
    black_model = BlackBox("j-hartmann/emotion-english-distilroberta-base")
    prompt = WordNetConditioner(hidden_dim=128, init_method='wordnet', black_model=black_model)  # ä¿®æ”¹ä¸ºä¸ diffusion æ¨¡å‹åŒ¹é…çš„ç»´åº¦
    embeddings_dict = prompt.embeddings_dict
    words_dict = prompt.embeddings_words_dict
    # print(embeddings_dict)
    print(words_dict)
    
