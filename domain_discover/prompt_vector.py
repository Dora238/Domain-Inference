"""Learnable prompt vector for domain discovery."""

import torch
import torch.nn as nn
import numpy as np
from typing import Optional, List, Dict, Tuple, Set, Union
from nltk.corpus import wordnet as wn
import nltk
import random
from collections import defaultdict
from pathlib import Path
from blackbox import BlackBox
from sklearn.cluster import KMeans
from sentence_transformers import SentenceTransformer

class SimpleEmbedder:
    """ç®€å•çš„æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼Œç”Ÿæˆ128ç»´åµŒå…¥å‘é‡
    
    è¿™ä¸ªç±»ä½¿ç”¨é¢„è®­ç»ƒçš„Sentence-BERTæ¨¡å‹ç”ŸæˆåµŒå…¥å‘é‡ï¼Œ
    å¹¶é€šè¿‡çº¿æ€§å±‚å°†å…¶æŠ•å½±åˆ°128ç»´ï¼Œä»¥ä¸diffusionæ¨¡å‹å…¼å®¹ã€‚
    
    Args:
        target_dim: ç›®æ ‡åµŒå…¥ç»´åº¦ï¼Œé»˜è®¤ä¸º128
        use_cache: æ˜¯å¦ç¼“å­˜åµŒå…¥ç»“æœ
    """
    
    def __init__(self, target_dim: int = 128, use_cache: bool = True):
        self.target_dim = target_dim
        self.use_cache = use_cache
        self.cache = {}
        
        # å»¶è¿ŸåŠ è½½æ¨¡å‹ï¼Œä»…åœ¨éœ€è¦æ—¶åŠ è½½
        self._model = None
        self._projection = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    def _load_model(self):
        """åŠ è½½é¢„è®­ç»ƒçš„Sentence-BERTæ¨¡å‹"""
        # ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ä»¥æé«˜æ•ˆç‡
        self._model = SentenceTransformer('paraphrase-MiniLM-L6-v2')
        # åˆ›å»ºæŠ•å½±å±‚
        input_dim = self._model.get_sentence_embedding_dimension()
        self._projection = nn.Linear(input_dim, self.target_dim).to(self.device)
        # éšæœºåˆå§‹åŒ–æŠ•å½±å±‚
        nn.init.xavier_uniform_(self._projection.weight)
        nn.init.zeros_(self._projection.bias)
        print(f"åŠ è½½äº†Sentence-BERTæ¨¡å‹å¹¶åˆ›å»ºäº†ä»{input_dim}åˆ°{self.target_dim}çš„æŠ•å½±å±‚")

    def encode(self, text: Union[str, List[str]]) -> torch.Tensor:
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºæŒ‡å®šç»´åº¦çš„åµŒå…¥å‘é‡
        
        Args:
            text: è¾“å…¥æ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            å½¢çŠ¶ä¸º(batch_size, target_dim)çš„åµŒå…¥å‘é‡
        """
        # å¦‚æœæ˜¯å•ä¸ªå­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
        if isinstance(text, str):
            texts = [text]
            single_input = True
        else:
            texts = text
            single_input = False
        
        # æ£€æŸ¥ç¼“å­˜
        if self.use_cache:
            # å°è¯•ä»ç¼“å­˜ä¸­è·å–åµŒå…¥
            cached_embeddings = []
            texts_to_encode = []
            indices = []
            
            for i, t in enumerate(texts):
                if t in self.cache:
                    cached_embeddings.append(self.cache[t])
                else:
                    texts_to_encode.append(t)
                    indices.append(i)
            
            # å¦‚æœæ‰€æœ‰æ–‡æœ¬éƒ½åœ¨ç¼“å­˜ä¸­
            if len(texts_to_encode) == 0:
                embeddings = torch.stack(cached_embeddings)
                if single_input:
                    return embeddings[0].unsqueeze(0)  # æ·»åŠ batchç»´åº¦
                return embeddings
        else:
            texts_to_encode = texts
            indices = list(range(len(texts)))
        
        # å»¶è¿ŸåŠ è½½æ¨¡å‹
        if self._model is None:
            self._load_model()
        
        # ç¼–ç æ–‡æœ¬
        with torch.no_grad():
            # ä½¿ç”¨Sentence-BERTè·å–åµŒå…¥
            embeddings = self._model.encode(texts_to_encode, convert_to_tensor=True)
            # ç¡®ä¿embeddingsåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            embeddings = embeddings.to(self.device)
            # æŠ•å½±åˆ°ç›®æ ‡ç»´åº¦
            embeddings = self._projection(embeddings)
        
        # æ›´æ–°ç¼“å­˜
        if self.use_cache:
            for t, emb in zip(texts_to_encode, embeddings):
                self.cache[t] = emb
            
            # å°†æ–°ç¼–ç çš„åµŒå…¥ä¸ç¼“å­˜çš„åµŒå…¥åˆå¹¶
            all_embeddings = [None] * len(texts)
            for i, emb in enumerate(cached_embeddings):
                # ç¡®ä¿ç¼“å­˜çš„åµŒå…¥ä¹Ÿåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                all_embeddings[i] = emb.to(self.device)
            for i, idx in enumerate(indices):
                all_embeddings[idx] = embeddings[i]
            
            embeddings = torch.stack(all_embeddings)
        
        # å¦‚æœæ˜¯å•ä¸ªè¾“å…¥ï¼Œè¿”å›å•ä¸ªåµŒå…¥
        if single_input:
            return embeddings[0].unsqueeze(0)  # æ·»åŠ batchç»´åº¦
        
        return embeddings


class PromptVector(nn.Module):
    """Learnable prompt vector for steering text generation.
    
    This module wraps a single learnable embedding vector that will be
    optimized to guide the diffusion model toward generating text with
    desired properties.
    
    Args:
        hidden_dim: Dimension of the embedding vector (matches model)
        init_scale: Scale for random initialization
        init_method: åˆå§‹åŒ–æ–¹æ³•ï¼Œå¯é€‰ 'random'(éšæœºåˆå§‹åŒ–) æˆ– 'wordnet'(åŸºäºWordNetåˆå§‹åŒ–)
        embedding_model: ç”¨äºå°†è¯æ±‡è½¬æ¢ä¸ºåµŒå…¥çš„æ¨¡å‹ï¼ˆå¦‚æœinit_method='wordnet'ï¼‰
        max_words: WordNetéå†çš„æœ€å¤§è¯æ±‡æ•°
        min_words_per_category: æ¯ä¸ªç±»åˆ«æœ€å°‘ä¿ç•™çš„è¯æ±‡æ•°é‡
    """
    
    def __init__(
        self,
        hidden_dim: int = 128,  # ä¿®æ”¹ä¸ºä¸ diffusion æ¨¡å‹åŒ¹é…çš„ç»´åº¦
        init_scale: float = 0.02,
        init_method: str = 'wordnet',
        embedding_model = None,
        max_words: int = 5000,
        min_words_per_category: int = 20,
        black_model = None,
    ):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.init_method = init_method
        self.black_model = black_model
        
        if init_method == 'random':
            # éšæœºåˆå§‹åŒ–
            self.embedding = nn.Parameter(
                torch.randn(1, hidden_dim) * init_scale
            )
        elif init_method == 'wordnet':
            # åŸºäºWordNetåˆå§‹åŒ–
            self.embedding_model = SimpleEmbedder()
            self.embeddings_dict, self.embeddings_words_dict = self.initialize_from_wordnet(black_model)
    
    def clone_detached(self) -> torch.Tensor:
        """Return a detached copy of the prompt vector.
        
        Returns:
            Tensor of shape (1, hidden_dim), detached from computation graph
        """
        return self.embedding.detach().clone()
    
    def initialize_from_wordnet(self, black_model = None) -> None:
        """ä»WordNetçš„è¯æ±‡åˆå§‹åŒ–prompt vector
        
        å¦‚æœæä¾›äº†black_modelå’Œtarget_labelï¼Œåˆ™æŸ¥æ‰¾é»‘ç›’æ¨¡å‹è¾“å‡ºä¸ºæŒ‡å®šæ ‡ç­¾çš„è¯æ±‡åµŒå…¥
        å¦åˆ™é€šè¿‡å¯¹ç‰¹å®šè¯­ä¹‰ç±»åˆ«çš„è¯æ±‡è¿›è¡ŒåµŒå…¥å¹¶å¹³å‡ï¼Œæ¥åˆå§‹åŒ–prompt vector
        
        Args:
            category: è¯­ä¹‰ç±»åˆ«åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™éšæœºé€‰æ‹©ä¸€ä¸ªç±»åˆ«
            sample_size: ä»ç±»åˆ«ä¸­é‡‡æ ·çš„è¯æ±‡æ•°é‡
            black_model: é»‘ç›’æ¨¡å‹ï¼Œç”¨äºé¢„æµ‹è¯æ±‡çš„æ ‡ç­¾
            target_label: ç›®æ ‡æ ‡ç­¾ï¼Œå¦‚æœæŒ‡å®šåˆ™æŸ¥æ‰¾é»‘ç›’æ¨¡å‹è¾“å‡ºä¸ºè¯¥æ ‡ç­¾çš„è¯æ±‡
        """
        
        # è·å–WordNetè¯æ±‡
        word_dict = traverse_wordnet(
            black_model=black_model,
            max_words=5000,
            min_words_per_label=20
            )
            
        # å¦‚æœæ‰¾åˆ°äº†ç›®æ ‡æ ‡ç­¾çš„è¯æ±‡
        embeddings_dict = {}
        embeddings_words_dict = {}
        for label, words in word_dict.items():
            print(f"æ‰¾åˆ° {len(words)} ä¸ªæ ‡ç­¾ä¸º {label} çš„è¯æ±‡")
            
            embeddings = []
            word_list = []

            for word in words:
                with torch.no_grad():
                    embedding = self.embedding_model.encode(word)
                    if isinstance(embedding, torch.Tensor):
                        if embedding.numel() == self.hidden_dim:  # ç¡®ä¿ç»´åº¦åŒ¹é…
                            embeddings.append(embedding.reshape(1, -1))
                            word_list.append(word)

            if len(embeddings) < 2:
                print(f"âš ï¸ ç±»åˆ« {label} ä¸­æœ‰æ•ˆåµŒå…¥è¯æ±‡æ•°é‡ä¸è¶³ï¼Œè·³è¿‡èšç±»")
                continue

            embeddings = torch.cat(embeddings, dim=0).cpu().numpy()

            # è®¾ç½®èšç±»ä¸ªæ•°ï¼ˆä½ å¯ä»¥è°ƒæ•´ï¼‰
            n_clusters = min(5, len(embeddings))
            kmeans = KMeans(n_clusters=n_clusters, random_state=42)
            kmeans.fit(embeddings)

            print(f"æ ‡ç­¾ {label} çš„èšç±»ç»“æœï¼ˆå…± {n_clusters} ç±»ï¼‰ï¼š")
            for cluster_id in range(n_clusters):
                indices = [i for i, pred in enumerate(kmeans.labels_) if pred == cluster_id]
                cluster_words = [word_list[i] for i in indices]
                print(f"  ğŸ”¸ Cluster {cluster_id}: {', '.join(cluster_words[:5])}...")
                embeddings_dict[label] = embeddings[indices]
                embeddings_words_dict[label] = cluster_words
        ## è¿”å›æ¯ä¸ªlabelå¯¹åº”çš„èšç±»çš„embeddings
        return embeddings_dict, embeddings_words_dict

def traverse_wordnet(black_model=None, max_words=5000, min_words_per_label=20):
    """éå†WordNetè¯æ±‡ï¼Œæ‰¾åˆ°é»‘ç›’æ¨¡å‹é¢„æµ‹ä¸ºç›®æ ‡æ ‡ç­¾çš„è¯æ±‡
    
    Args:
        black_model: é»‘ç›’æ¨¡å‹ï¼Œç”¨äºé¢„æµ‹è¯æ±‡çš„æ ‡ç­¾
        max_words: æœ€å¤§å¤„ç†è¯æ±‡æ•°é‡
        min_words_per_label: æ¯ä¸ªæ ‡ç­¾æœ€å°‘ä¿ç•™çš„è¯æ±‡æ•°é‡
        
    Returns:
        å¦‚æœtarget_labelä¸ºNoneï¼Œè¿”å›{label: [words]}çš„å­—å…¸
        å¦‚æœtarget_labelä¸ä¸ºNoneï¼Œè¿”å›ç¬¦åˆç›®æ ‡æ ‡ç­¾çš„è¯æ±‡åˆ—è¡¨
    """
    
    # è·å–æ‰€æœ‰åŒä¹‰è¯é›†
    all_synsets = list(wn.all_synsets())
    print(f"WordNetä¸­å…±æœ‰ {len(all_synsets)} ä¸ªåŒä¹‰è¯é›†")
    
    label_to_words = defaultdict(set)
    count = 0

    for synset in all_synsets[:max_words]:
        for lemma in synset.lemma_names():
            word = lemma.replace('_', ' ')  # å°†ä¸‹åˆ’çº¿æ›¿æ¢ä¸ºç©ºæ ¼ï¼Œä½¿å…¶æ›´è‡ªç„¶
            try:
                pred_label = black_model.predict([word])[0]
                label_to_words[pred_label].add(word)
            except Exception as e:
                print(f"âš ï¸ Prediction failed for word: {word} | Error: {e}")
                continue

            count += 1
            if count % 200 == 0:
                current_status = {k: len(v) for k, v in label_to_words.items()}
                print(f"Processed {count} samples; label stats: {current_status}")

        # å¦‚æœæ‰€æœ‰æ ‡ç­¾éƒ½æ»¡è¶³æœ€å°è¯æ•°è¦æ±‚ï¼Œåˆ™æå‰ç»ˆæ­¢
        if all(len(label_to_words[label]) >= min_words_per_label for label in range(black_model.num_labels)):
            break

    # æœ€ç»ˆè¿‡æ»¤ä¸æ»¡è¶³è¦æ±‚çš„æ ‡ç­¾
    filtered_result = {
        label: words for label, words in label_to_words.items()
        if len(words) >= min_words_per_label
    }

    print(f"æœ€ç»ˆä¿ç•™æ ‡ç­¾æ•°: {len(filtered_result)}")
    return filtered_result


if __name__ == "__main__":

    # æµ‹è¯•éšæœºåˆå§‹åŒ–
    print("\næµ‹è¯•éšæœºåˆå§‹åŒ–...")
    black_model = BlackBox("j-hartmann/emotion-english-distilroberta-base")
    prompt = PromptVector(hidden_dim=128, init_method='wordnet', black_model=black_model)  # ä¿®æ”¹ä¸ºä¸ diffusion æ¨¡å‹åŒ¹é…çš„ç»´åº¦
    embeddings_dict = prompt.embeddings_dict
    words_dict = prompt.embeddings_words_dict
    # print(embeddings_dict)
    print(words_dict)
    
